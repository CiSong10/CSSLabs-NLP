{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Lab\n",
    "\n",
    "### To do list:\n",
    "   \n",
    "- To-do:\n",
    "    - interpreting topics (out of notebook placeholder)\n",
    "    - top topics by personal attributes\n",
    "    - comparing LDA & NMF topics (deal with alignment)\n",
    "    - credit to https://medium.com/@aneesha/topic-modeling-with-scikit-learn-e80d33668730\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "### Step 1: Import the packages we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.interpolate import griddata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import normalize\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read in our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = pd.read_csv('data/clean_profiles.tsv', sep='\\t')\n",
    "profiles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Pick which section of the profiles you want to analyze.\n",
    "#### Options:\n",
    "- text - All of the text from a profile\n",
    "- essay0 - My self summary\n",
    "- essay1 - What I’m doing with my life\n",
    "- essay2 - I’m really good at\n",
    "- essay3 - The first thing people usually notice about me\n",
    "- essay4 - Favorite books, movies, show, music, and food\n",
    "- essay5 - The six things I could never do without\n",
    "- essay6 - I spend a lot of time thinking about\n",
    "- essay7 - On a typical Friday night I am\n",
    "- essay8 - The most private thing I am willing to admit\n",
    "- essay9 - You should message me if...\n",
    "\n",
    "#### Replace `'essay0'` in the cell below with the essay you want to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profile_section_to_use = 'essay0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Clean up the text for that essay.\n",
    "#### Helper function for cleaning up text\n",
    "- removes HTML code, link artefacts\n",
    "- converts to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some of the essays have just a link in the text. BeautifulSoup sees that and gets \n",
    "# the wrong idea. This line hides those warnings.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "def clean(text):\n",
    "    if pd.isnull(text):\n",
    "        t = np.nan\n",
    "    else:\n",
    "        t = BeautifulSoup(text, 'lxml').get_text()\n",
    "        t = t.lower()\n",
    "\n",
    "        bad_words = ['http', 'www', '\\nnan']\n",
    "\n",
    "        for b in bad_words:\n",
    "            t = t.replace(b, '')\n",
    "    if t == '':\n",
    "        t = np.nan\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean and select the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning up profile text for', profile_section_to_use, '...')\n",
    "profiles['clean'] = profiles[profile_section_to_use].apply(clean)\n",
    "\n",
    "print('We started with', profiles.shape[0], 'profiles.')\n",
    "print(\"Dropping profiles that didn't fill out the essay we chose...\")\n",
    "profiles.dropna(axis=0, subset=['clean'], inplace=True)\n",
    "\n",
    "print('We have', profiles.shape[0], 'profiles left.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Topic Modeling\n",
    "#### Some parameters: change these to get different numbers of topics or words per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many topics we want our model to find\n",
    "ntopics = 20\n",
    "\n",
    "#how many top words we want to display for each topic\n",
    "nshow = 10\n",
    "\n",
    "#what we will use as our documents, here the cleaned up text of each profile\n",
    "documents = profiles['clean'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 LDA\n",
    "### Step 1: Convert text to numbers the computer understands\n",
    "- LDA takes \"count vectors\" as input, that is, a count of how many times each word shows up in each document. \n",
    "    - Here we tell it to only use the 1,000 most popular words, ignoring stop words\n",
    "- [Learn more](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) about LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "print(\"Vectorizing text by word counts...\")\n",
    "tf_text = tf_vectorizer.fit_transform(documents)\n",
    "\n",
    "tmp = tf_text.get_shape()\n",
    "print(\"Our transformed text has\", tmp[0], \"rows and\", tmp[1], \"columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "print(\"The first few words (alphabetically) are:\\n\", tf_feature_names[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build a topic model using LDA\n",
    "\n",
    "- LDA can be a little slow. We'll use a faster method later on.\n",
    "- Set `n_jobs=` to the number of processors you want to use to compute LDA. If you set it to `-1`, it will use all available processors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentDirichletAllocation(n_components=ntopics, max_iter=10, \n",
    "                                  learning_method='online', n_jobs=-1)\n",
    "\n",
    "print('Performing LDA on vectors...')\n",
    "lda = model.fit(tf_text)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to sort and show us the most important words in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def describe_topic(topic, feature_names, n_words=10):\n",
    "    words = []\n",
    "    # sort the words in the topic by importance\n",
    "    topic = topic.argsort() \n",
    "    # select the n_words most important words\n",
    "    topic = topic[:-n_words - 1:-1]\n",
    "    # for each important word, get it's name (i.e. the word) from our list of names\n",
    "    for i in topic:\n",
    "        words.append(feature_names[i])\n",
    "    # print the topic number and its most important words, separated by spaces\n",
    "    return \" \".join(words)\n",
    "\n",
    "def display_topics(components, feature_names, n_words=10):\n",
    "    # loop through each topic (component) in the model; show its top words\n",
    "    for topic_idx, topic in enumerate(components):\n",
    "        print(\"Topic {}:\".format(topic_idx), describe_topic(topic, feature_names, n_words))\n",
    "    return\n",
    "\n",
    "def sort_topics(components):\n",
    "    # run hierarchical clustering to find related groups of topics\n",
    "    l = linkage(components, \"ward\")\n",
    "    # calculate the id of the final cluster\n",
    "    last_id = 2 * ntopics - 2\n",
    "    # start with the final cluster and break it into smaller clusters\n",
    "    order = [last_id]\n",
    "    for i, row in reversed(list(enumerate(l))):\n",
    "        # find the current cluster in the list and break it into two smaller clusters\n",
    "        cluster_id = ntopics + i\n",
    "        index = order.index(cluster_id)\n",
    "        order = order[:index] + [row[0], row[1]] + order[(index+1):]\n",
    "    # sort topics by the order calcuated above and return a copy\n",
    "    components = [x[1] for x in sorted(zip(order, components))]\n",
    "    return np.array(components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to compare different topics to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similarity(a, b=None):\n",
    "    # normalize the topics so that their dot product is 1\n",
    "    a = normalize(a)\n",
    "    # if only one set of topics is given, compare it to itself\n",
    "    if b is None:\n",
    "        b = a\n",
    "    else:\n",
    "        b = normalize(b)\n",
    "    # create a 2-D array to store the results of the similarity calcluation\n",
    "    topic_similarity = np.zeros((ntopics, ntopics))\n",
    "    # loop through each topic in both a and b\n",
    "    for topic_idx, row in enumerate(a):\n",
    "        for topic_jdx, col in enumerate(b):\n",
    "            # calculate the similarity using a dot product\n",
    "            topic_similarity[topic_idx, topic_jdx] = np.inner(row, col)\n",
    "    return topic_similarity\n",
    "\n",
    "def describe_intersection(components, a, b, feature_names, n_words=10):\n",
    "    # normalize\n",
    "    topic_a = components[a,:] / components[a,:].sum()\n",
    "    topic_b = components[b,:] / components[b,:].sum()\n",
    "    # multiply components of a and b to highlight words common in both\n",
    "    x = topic_a * topic_b\n",
    "    print(\"Words in {} and {}:\".format(a, b), describe_topic(x, feature_names, n_words))\n",
    "\n",
    "def describe_difference(components, a, b, feature_names, n_words=10):\n",
    "    # normalize\n",
    "    topic_a = components[a,:] / components[a,:].sum()\n",
    "    topic_b = components[b,:] / components[b,:].sum()\n",
    "    # multiply components of a with complement of components in b\n",
    "    x = topic_a * (1 - topic_b)\n",
    "    print(\"Words in {} but not in {}:\".format(a, b), describe_topic(x, feature_names, n_words))\n",
    "\n",
    "def plot_topics(components):\n",
    "    # sort topics into similar groups\n",
    "    components = sort_topics(components)\n",
    "    # calculate similarity between topics\n",
    "    topic_similarity = get_similarity(components)\n",
    "    # create a figure and plot the similarites\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    plt.imshow(np.log10(topic_similarity), cmap='Blues')\n",
    "    # move the ticks to the top and maker sure each topic is labeled\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.xticks(range(20)); plt.yticks(range(20))\n",
    "    # plot a colorbar legend\n",
    "    plt.colorbar()\n",
    "    \n",
    "def plot_confusion(x, y):\n",
    "    # normalize all topics so their inner product is 1\n",
    "    a = normalize(y)\n",
    "    b = normalize(x)\n",
    "    # match topics in b with their most similar topic in a\n",
    "    sorted_b = []\n",
    "    order_b = []\n",
    "    idx_b = np.arange(ntopics)\n",
    "    for ta in a:\n",
    "        # of the b topics not yet assigned, find the one that best matches\n",
    "        best_b = max([(np.inner(ta, b[i,:]), i) for i in range(b.shape[0])])[1]\n",
    "        # move the b topic into the sorted list\n",
    "        sorted_b.append(b[best_b,:])\n",
    "        order_b.append(idx_b[best_b])\n",
    "        b = np.delete(b, best_b, axis=0)\n",
    "        idx_b = np.delete(idx_b, best_b, axis=0)\n",
    "    # replace b topics with the sorted version\n",
    "    b = np.array(sorted_b)\n",
    "    # find similarity\n",
    "    topic_similarity = get_similarity(a, b)\n",
    "    # create a figure and plot the similarity valuse\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    plt.imshow(np.log10(topic_similarity), cmap='Blues')\n",
    "    # move the ticks to the top and reorder the x ticks because b was sorted\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.xticks(range(20), order_b)\n",
    "    plt.yticks(range(20))\n",
    "    # show a colorbar legend\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Show our topics with the top words in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics = sort_topics(lda.components_)\n",
    "display_topics(lda_topics, tf_feature_names, n_words=nshow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Compare topics to each other\n",
    "\n",
    "We can compare topics visually by plotting the similarity of each topic to each other topic.\n",
    "Groups of similar topics are placed closer together.\n",
    "- Why are the diagonal cells the darkest?\n",
    "- What do dark or light bands represent?\n",
    "- Do you see dark regions made of several cells? If so, what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topics(lda_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the words that make two topics similar or different\n",
    "We can also compare two topics to each other by looking at words that are common in both,\n",
    "or words that are common in one but not the other.\n",
    "Try changing `topic_a` and `topic_b` to different topic numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_a = 0\n",
    "topic_b = 3\n",
    "\n",
    "describe_intersection(lda_topics, topic_a, topic_b, tf_feature_names, n_words=nshow)\n",
    "describe_difference(lda_topics, topic_a, topic_b, tf_feature_names, n_words=nshow)\n",
    "describe_difference(lda_topics, topic_b, topic_a, tf_feature_names, n_words=nshow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Interpret these topics\n",
    "- This part is for you to do: code can't do it for you.\n",
    "- Look at the list of important words for each topic, and think about these questions.\n",
    "    - What do the words have in common?\n",
    "    - What could someone write that would use most of those words?\n",
    "    - What does this topic seem to be about?\n",
    "- Try to come up with a short, catchy name for each topic.\n",
    "    - For example, if the words were \"san francisco city moved living born years raised lived live\", you might call it \"places lived\" because the topic seems to be about where people currently live (San Francisco) and where they were born / raised / moved from. \n",
    "- Try other numbers of topics.\n",
    "    - If the topics seem repetitive, you might want to try looking for fewer topics.\n",
    "    - If the topics seem confusing or vague, you might want to try looking for more topics (so that they can be more specific)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 NMF\n",
    "### Step 1: Convert text to numbers the computer understands\n",
    "- NMF takes \"tf-idf vectors\" as input. Tf-idf stands for \"text frequency - inverse document frequency.\" \n",
    "    - Text frequency is the same as the count vectors above: how often does each word appear in the text. \n",
    "    - Inverse document frequency means we divide (\"inverse\") by the number of documents the word is in. (If everyone uses the word, it isn't very helpful for figuring out what different people are talking about. So this measurement looks for words that are used a lot in some documents, and not at all in others.)\n",
    "    - Here we tell it to only use the 1,000 most popular words, ignoring stop words\n",
    "- [Learn more](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization#Text_mining) about NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "print(\"Vectorizing text by TF-IDF...\")\n",
    "tfidf_text = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "tmp = tfidf_text.get_shape()\n",
    "print(\"Our transformed text has\", tmp[0], \"rows and\", tmp[1], \"columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The features are the same, because they are just the list of words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print(\"The first few words (alphabetically) are:\\n\", tfidf_feature_names[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build a topic model using NMF\n",
    "\n",
    "- NMF is faster than LDA and often works a little better for small documents like we have here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=ntopics, alpha=.1, l1_ratio=.5, init='nndsvd')\n",
    "\n",
    "print('Performing NMF on vectors...')\n",
    "nmf = model.fit(tfidf_text)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Show our topics with the top words in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_topics = sort_topics(nmf.components_)\n",
    "display_topics(nmf_topics, tfidf_feature_names, nshow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Compare topics to each other\n",
    "We can compare topics visually by plotting the similarity of each topic to each other topic. Groups of similar topics are placed closer together because we sorted them above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topics(nmf_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the words that make two topics similar or different\n",
    "We can also compare two topics to each other by looking at words that are common in both, or words that are common in one but not the other. Try changing topic_a and topic_b to different topic numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_a = 0\n",
    "topic_b = 1\n",
    "\n",
    "describe_intersection(nmf_topics, topic_a, topic_b, tfidf_feature_names, n_words=nshow)\n",
    "describe_difference(nmf_topics, topic_a, topic_b, tfidf_feature_names, n_words=nshow)\n",
    "describe_difference(nmf_topics, topic_b, topic_a, tfidf_feature_names, n_words=nshow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 5: Interpret these topics\n",
    "- This part is for you to do: code can't do it for you.\n",
    "- Look at the list of important words for each topic, and think about these questions.\n",
    "    - What do the words have in common?\n",
    "    - What could someone write that would use most of those words?\n",
    "    - What does this topic seem to be about?\n",
    "- Try to come up with a short, catchy name for each topic.\n",
    "    - For example, if the words were \"san francisco city moved living born years raised lived live\", you might call it \"places lived\" because the topic seems to be about where people currently live (San Francisco) and where they were born / raised / moved from. \n",
    "- Try other numbers of topics.\n",
    "    - If the topics seem repetitive, you might want to try looking for fewer topics.\n",
    "    - If the topics seem confusing or vague, you might want to try looking for more topics (so that they can be more specific)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 6: Compare the topics from LDA and NMF\n",
    "We can compare the topics visually using a confusion matrix plot.\n",
    "The NMF topics are along the X axis and the LDA are along the Y axis.\n",
    "The NMF topics are sorted to match the closest LDA topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion(x=nmf_topics, y=lda_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the LDA and NMF topic words and the confusion matrix, consider the following questions:\n",
    "- Do any of the topics seem to be the same in both models?\n",
    "- Are some topics in one model but not the other?\n",
    "- Do the topics you get from one of the models make more sense than the ones you get from the other?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Compare profiles based on their topics\n",
    "For each profile, we calculate how strongly each topic appears.\n",
    "This code uses the NMF model.\n",
    "To use LDA instead, remove the `#` at the beginning of the second line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model, text = nmf, tfidf_text\n",
    "#model, text = lda, tf_text\n",
    "model_doc_topic = model.transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to visualize and compare topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_profile(model_doc_topic, profile_id):\n",
    "    # plot a stem diagram for a single profile\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.xticks(range(ntopics))\n",
    "    plt.xlabel('Topic number')\n",
    "    plt.ylabel('Proportion of profile about this topic')\n",
    "    plt.stem(model_doc_topic[profile_id,:])\n",
    "    \n",
    "def profile_histogram(model_doc_topic, topic_id):\n",
    "    # get profile values within a single topic\n",
    "    values = model_doc_topic[:,topic_id]\n",
    "    # calculate logarithmic bins based on smallest nonzero value\n",
    "    bins = np.logspace(np.log10(min([v for v in values if v > 0])),np.log10(0.4),50)\n",
    "    # plot the histogram\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.ylabel('Number of profiles')\n",
    "    plt.xlabel('Amount of topic {} in profile'.format(topic_id))\n",
    "    plt.hist(values, bins=bins)\n",
    "    plt.gca().set_xscale('log')\n",
    "\n",
    "def profile_scatter(model_doc_topic, topic_x_id, topic_y_id):\n",
    "    # create a scatter plot of profiles with each axis representing one topic\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.loglog(model_doc_topic[:,topic_x_id], model_doc_topic[:,topic_y_id], '.', markersize=1)\n",
    "    plt.xlabel('Amount of topic {}'.format(topic_x_id))\n",
    "    plt.ylabel('Amount of topic {}'.format(topic_y_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the topic breakdown of particular profiles\n",
    "Examine different profiles by changing the value of the `profile_id` varible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_profile(model_doc_topic, profile_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize all of the profiles by their topic content\n",
    "We can choose a topic and plot a histogram of that topic's relevance to each of the profiles.\n",
    "- Try different topics by changing `topic_id`.\n",
    "- Are there any topics with multiple peaks? What does that mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_histogram(model_doc_topic, topic_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_histogram(model_doc_topic, topic_id=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the relationship between two topics using a scatter plot of all profiles\n",
    "- Try different topic combinations.\n",
    "- Do the profiles cluster into distinct groups?\n",
    "- What are the pros and cons of using a 2-D scatter plot vs the histogram above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_scatter(model_doc_topic, topic_x_id=0, topic_y_id=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going Further\n",
    "Here are some ideas for doing a more detailed exploration of the data.\n",
    "- Are certain topics correlated with age, sex, education, or other demographic data?\n",
    "- Are any topics opposites of each other?\n",
    "- Visualize profiles for different demographics in different colors.\n",
    "- What are the profiles that most exemplify each topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
