{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Lab\n",
    "\n",
    "- In this lab, we'll learn about topic modeling. Topic modeling uses statistics to understand what text is about, that is, to find the topics in text.\n",
    "- We'll use the online dating profile text that OKCupid made public as our example, but of course topic modeling can be used on any text.\n",
    "- (Some of the code for this lab was adapted from [Aneesha Bakharia](https://medium.com/@aneesha/topic-modeling-with-scikit-learn-e80d33668730)'s example.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup and cleaning\n",
    "### Step 1: Import the packages we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.interpolate import griddata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import normalize\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read in our data \n",
    "\n",
    "#### Important note: \n",
    "Before loading this data, you **must** run the code from section `0. Setup` in Lab `1. What is in a dating profile?`. This code prepares the data we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = pd.read_csv('data/clean_profiles.tsv', sep='\\t')\n",
    "profiles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Pick which section of the profiles you want to analyze.\n",
    "#### Options:\n",
    "- text - All of the text from a profile\n",
    "- essay0 - My self summary\n",
    "- essay1 - What I’m doing with my life\n",
    "- essay2 - I’m really good at\n",
    "- essay3 - The first thing people usually notice about me\n",
    "- essay4 - Favorite books, movies, show, music, and food\n",
    "- essay5 - The six things I could never do without\n",
    "- essay6 - I spend a lot of time thinking about\n",
    "- essay7 - On a typical Friday night I am\n",
    "- essay8 - The most private thing I am willing to admit\n",
    "- essay9 - You should message me if...\n",
    "\n",
    "#### Replace `'essay0'` in the cell below with the essay you want to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_section_to_use = 'essay0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Clean up the text for that essay.\n",
    "#### Helper function for cleaning up text\n",
    "- removes HTML code, link artifacts\n",
    "- converts to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the essays have just a link in the text. BeautifulSoup sees that and gets \n",
    "# the wrong idea. This line hides those warnings.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "def clean(text):\n",
    "    if pd.isnull(text):\n",
    "        t = np.nan\n",
    "    else:\n",
    "        t = BeautifulSoup(text, 'lxml').get_text()\n",
    "        t = t.lower()\n",
    "\n",
    "        bad_words = ['http', 'www', '\\nnan']\n",
    "\n",
    "        for b in bad_words:\n",
    "            t = t.replace(b, '')\n",
    "    if t == '':\n",
    "        t = np.nan\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean and select the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning up profile text for', profile_section_to_use, '...')\n",
    "profiles['clean'] = profiles[profile_section_to_use].apply(clean)\n",
    "\n",
    "print('We started with', profiles.shape[0], 'profiles.')\n",
    "print(\"Dropping profiles that didn't write anything for the essay we chose...\")\n",
    "profiles.dropna(axis=0, subset=['clean'], inplace=True)\n",
    "\n",
    "#what we will use as our documents, here the cleaned up text of each profile\n",
    "documents = profiles['clean'].values\n",
    "\n",
    "print('We have', profiles.shape[0], 'profiles left.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Converting text to numbers for a topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Convert text to numbers the computer understands\n",
    "- Our first model takes \"count vectors\" as input, that is, a count of how many times each word shows up in each document. \n",
    "    - Here we tell it to only use the 1,000 most popular words, ignoring stop words like \"a\" and \"of\".\n",
    "    - We use the abbreviation `tf` for these because they represent \"text frequency,\" i.e., how often each word shows up in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "print(\"Vectorizing text by word counts...\")\n",
    "tf_text = tf_vectorizer.fit_transform(documents)\n",
    "\n",
    "tmp = tf_text.get_shape()\n",
    "print(\"Our transformed text has\", tmp[0], \"rows and\", tmp[1], \"columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See what words are being counted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "print(\"The first few words (alphabetically) are:\\n\", tf_feature_names[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See an example of how a profile's text is encoded\n",
    "- Change the value of `n` and re-run the code to see different profiles.\n",
    "- Note that only some of the words are counted. This is because we set `max_features=1000` in the vectorizor function, so it is only counting the 1,000 most common words and ignoring the rest. \n",
    "    - You can change that number to be bigger or smaller and see what happens.\n",
    "    - We found in Lab 1 that 1,000 is a good choice for this data because words less popular than that show up in less than 1% of all profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6\n",
    "\n",
    "def show_vector(x):\n",
    "    rows,cols = x.nonzero()\n",
    "    for row,col in zip(rows,cols):\n",
    "        print(tf_feature_names[col], x[row,col])\n",
    "\n",
    "print('Profile text:\\n', documents[n])\n",
    "print('\\nTF (count) vector:')\n",
    "show_vector(tf_text[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build a topic model using LDA\n",
    "\n",
    "- LDA stands for Latent Dirichlet Allocation. The statistical math behind it is complicated, but its goals are simple:\n",
    "    - find groups of words that often show up together and call those groups topics. \n",
    "    - find topics that can be used to tell documents apart, i.e. topics that are in some documents but not others.\n",
    "- LDA is the most popular method for topic modeling.\n",
    "- We must tell LDA how many topics we want it to look for (we did this above with the `ntopics` variable.\n",
    "- LDA can be a little slow. We'll use a faster method later on.\n",
    "- Set `n_jobs=` to the number of processors you want to use to compute LDA. If you set it to `-1`, it will use all available processors. \n",
    "- [Learn more](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) about LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many topics we want our model to find\n",
    "ntopics = 15\n",
    "\n",
    "#how many top words we want to display for each topic\n",
    "nshow = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentDirichletAllocation(n_components=ntopics, max_iter=10, \n",
    "                                  learning_method='online', n_jobs=-1)\n",
    "\n",
    "print('Performing LDA on vectors...')\n",
    "lda = model.fit(tf_text)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Interpret the topics the model gives us\n",
    "#### Some helper functions \n",
    "Don't worry about how these work right now. We'll use them to make our analysis easier later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_topic(topic, feature_names, n_words=10):\n",
    "    words = []\n",
    "    # sort the words in the topic by importance\n",
    "    topic = topic.argsort() \n",
    "    # select the n_words most important words\n",
    "    topic = topic[:-n_words - 1:-1]\n",
    "    # for each important word, get it's name (i.e. the word) from our list of names\n",
    "    for i in topic:\n",
    "        words.append(feature_names[i])\n",
    "    # print the topic number and its most important words, separated by spaces\n",
    "    return \" \".join(words)\n",
    "\n",
    "def display_topics(components, feature_names, n_words=10):\n",
    "    # loop through each topic (component) in the model; show its top words\n",
    "    for topic_idx, topic in enumerate(components):\n",
    "        print(\"Topic {}:\".format(topic_idx), \n",
    "              describe_topic(topic, feature_names, n_words))\n",
    "    return\n",
    "\n",
    "def sort_topics(components):\n",
    "    n = components.shape[0]\n",
    "    # run hierarchical clustering to find related groups of topics\n",
    "    l = linkage(components, \"ward\")\n",
    "    # calculate the id of the final cluster\n",
    "    last_id = 2 * n - 2\n",
    "    # start with the final cluster and break it into smaller clusters\n",
    "    order = [last_id]\n",
    "    for i, row in reversed(list(enumerate(l))):\n",
    "        # find the current cluster in the list and break it into two smaller clusters\n",
    "        cluster_id = n + i\n",
    "        index = order.index(cluster_id)\n",
    "        order = order[:index] + [row[0], row[1]] + order[(index+1):]\n",
    "    # sort topics by the order calcuated above and return a copy\n",
    "    components = [x[1] for x in sorted(zip(order, components))]\n",
    "    return np.array(components)\n",
    "\n",
    "def get_similarity(a, b=None):\n",
    "    n = a.shape[0]\n",
    "    # normalize the topics so that their dot product is 1\n",
    "    a = normalize(a)\n",
    "    # if only one set of topics is given, compare it to itself\n",
    "    if b is None:\n",
    "        b = a\n",
    "    else:\n",
    "        b = normalize(b)\n",
    "    # create a 2-D array to store the results of the similarity calcluation\n",
    "    topic_similarity = np.zeros((n, n))\n",
    "    # loop through each topic in both a and b\n",
    "    for topic_idx, row in enumerate(a):\n",
    "        for topic_jdx, col in enumerate(b):\n",
    "            # calculate the similarity using a dot product\n",
    "            topic_similarity[topic_idx, topic_jdx] = np.inner(row, col)\n",
    "    return topic_similarity\n",
    "\n",
    "def describe_intersection(components, a, b, feature_names, n_words=10):\n",
    "    # normalize\n",
    "    topic_a = components[a,:] / components[a,:].sum()\n",
    "    topic_b = components[b,:] / components[b,:].sum()\n",
    "    # multiply components of a and b to highlight words common in both\n",
    "    x = topic_a * topic_b\n",
    "    print(\"Words in {} and {}:\".format(a, b), \n",
    "          describe_topic(x, feature_names, n_words))\n",
    "\n",
    "def describe_difference(components, a, b, feature_names, n_words=10):\n",
    "    # normalize\n",
    "    topic_a = components[a,:] / components[a,:].sum()\n",
    "    topic_b = components[b,:] / components[b,:].sum()\n",
    "    # multiply components of a with complement of components in b\n",
    "    x = topic_a * (1 - topic_b)\n",
    "    print(\"Words in {} but not in {}:\".format(a, b), \n",
    "          describe_topic(x, feature_names, n_words))\n",
    "\n",
    "def plot_topics(components):\n",
    "    n = components.shape[0]\n",
    "    # sort topics into similar groups\n",
    "    components = sort_topics(components)\n",
    "    # calculate similarity between topics\n",
    "    topic_similarity = get_similarity(components)\n",
    "    # create a figure and plot the similarites\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    plt.imshow(np.log10(topic_similarity), cmap='Blues')\n",
    "    # move the ticks to the top and maker sure each topic is labeled\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.xticks(range(n))\n",
    "    plt.yticks(range(n))\n",
    "    # plot a colorbar legend\n",
    "    plt.colorbar()\n",
    "    \n",
    "def plot_confusion(x, y):\n",
    "    \n",
    "    # TODO: switch this to best overall match rather than greedy match \n",
    "    \n",
    "    # normalize all topics so their inner product is 1\n",
    "    a = normalize(y)\n",
    "    b = normalize(x)\n",
    "    # match topics in b with their most similar topic in a\n",
    "    sorted_b = []\n",
    "    order_b = []\n",
    "    idx_b = np.arange(x.shape[0])\n",
    "    for ta in a:\n",
    "        # of the b topics not yet assigned, find the one that best matches\n",
    "        best_b = max([(np.inner(ta, b[i,:]), i) for i in range(b.shape[0])])[1]\n",
    "        # move the b topic into the sorted list\n",
    "        sorted_b.append(b[best_b,:])\n",
    "        order_b.append(idx_b[best_b])\n",
    "        b = np.delete(b, best_b, axis=0)\n",
    "        idx_b = np.delete(idx_b, best_b, axis=0)\n",
    "    # replace b topics with the sorted version\n",
    "    b = np.array(sorted_b)\n",
    "    # find similarity\n",
    "    topic_similarity = get_similarity(a, b)\n",
    "    # create a figure and plot the similarity valuse\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    plt.imshow(np.log10(topic_similarity), cmap='Blues')\n",
    "    # move the ticks to the top and reorder the x ticks because b was sorted\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.xticks(range(20), order_b)\n",
    "    plt.yticks(range(20))\n",
    "    # show a colorbar legend\n",
    "    plt.colorbar()\n",
    "    \n",
    "def get_profiles_from_topics(data, transformed, topic_a, topic_b=None, pick_from=10):\n",
    "    df = pd.DataFrame(transformed)\n",
    "    df = df.sort_values(by=topic_a, ascending=False)\n",
    "    match_text = ''\n",
    "    \n",
    "    if topic_b is None:\n",
    "        keep = df.head(pick_from).sample(1)\n",
    "        pid = keep.index.values[0]\n",
    "        match_text = 'is the number ' + str(np.where(df.index==pid)[0][0])\n",
    "        match_text += ' profile for topic ' + str(topic_a) \n",
    "    else:\n",
    "        df2 = df.sort_values(by=topic_b, ascending=False)\n",
    "        both = set()\n",
    "        idxs = {'a': df.index.values, 'b': df2.index.values}\n",
    "        seen = {'a': set(), 'b': set()}\n",
    "        i = 0\n",
    "        while len(both) < pick_from:\n",
    "            seen['a'].add(idxs['a'][i])\n",
    "            seen['b'].add(idxs['b'][i])\n",
    "            both = seen['a'].intersection(seen['b'])\n",
    "            i += 1\n",
    "        keep = df.loc[list(both), :].sample(1)\n",
    "        pid = keep.index.values[0]        \n",
    "        \n",
    "        match_text = 'is the number ' + str(np.where(df.index==pid)[0][0])\n",
    "        match_text += ' profile for topic ' + str(topic_a)\n",
    "        match_text += ' and the number ' + str(np.where(df2.index==pid)[0][0])\n",
    "        match_text += ' profile for topic ' + str(topic_b)\n",
    "        \n",
    "    text = data[pid]\n",
    "    print('Profile number', pid, match_text)\n",
    "    print('Here is the text:\\n\\n', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Show our topics with the top words in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics = sort_topics(lda.components_)\n",
    "display_topics(lda_topics, tf_feature_names, n_words=nshow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Examine the words that make two topics similar or different\n",
    "We can also compare two topics to each other by looking at words that are common in both,\n",
    "or words that are common in one but not the other.\n",
    "Try changing `topic_a` and `topic_b` to different topic numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_a = 8\n",
    "topic_b = 9\n",
    "\n",
    "describe_intersection(lda_topics, topic_a, topic_b, tf_feature_names, n_words=nshow)\n",
    "describe_difference(lda_topics, topic_a, topic_b, tf_feature_names, n_words=nshow)\n",
    "describe_difference(lda_topics, topic_b, topic_a, tf_feature_names, n_words=nshow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: See how similar the topics' words are\n",
    "\n",
    "We can compare topics visually by plotting the similarity of each topic to each other topic. How to interpret:\n",
    "- Each square shows how similar two topics are. Darker means more similar, and lighter means more different.\n",
    "- The square in the very top right shows how similar topic 0 is to topic 0 (i.e. how similar it is to itself). \n",
    "- The square next to it in the top row shows how similar topic 0 is to topic 1, and so on. \n",
    "- For any two topics, you can see how similar they are by finding their numbers on the edge and seeing where they intersect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topics(lda_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: See how often topics show up together in the same profiles\n",
    "\n",
    "- Why are the diagonal cells the darkest?\n",
    "- What do dark or light bands represent?\n",
    "- Do you see dark regions made of several cells? If so, what do they represent?\n",
    "\n",
    "#### Step 4a: For each profile, calculate how much it seems to be about each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_doc_topic = lda.transform(tf_text)\n",
    "#pd.DataFrame(model_doc_topic).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4b: See which topics tend to appear together in text\n",
    "- This shows us something that looks similar to the topic similarity we saw before, but this time:\n",
    "    - We don't compare topics based on which words they use\n",
    "    - We do compare topics based on how often they appear in the same profile as one another\n",
    "- Why is the diagonal line so dark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def topic_cooccurance(topics):\n",
    "    n = topics.shape[1]\n",
    "    m = pd.DataFrame(topics).corr()\n",
    "    # create a figure and plot the similarites\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    plt.imshow(m, cmap='Blues')\n",
    "    plt.xticks(range(n))\n",
    "    plt.yticks(range(n))\n",
    "    plt.title('Topic co-occurance (correlation in profile data)')\n",
    "    # plot a colorbar legend\n",
    "    plt.colorbar()\n",
    "\n",
    "topic_cooccurance(model_doc_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the topics are mostly uncorrelated. \n",
    "- The cells in the figure above are mostly very light blue\n",
    "- This doesn't mean that, for instance, topic 1 and 2 never show up in the same profile.\n",
    "- It does mean, however, that seeing any particular topic doesn't mean we're especially likely to also see any other topic.\n",
    "\n",
    "#### Just because they're uncorrelated doesn't mean there isn't a pattern in the relationship between two topics\n",
    "- This makes a scatterplot where each dot is a profile\n",
    "- The dot's X and Y position show how much of a profile is about each topic\n",
    "- Try different topic combinations.\n",
    "- Do the profiles cluster into distinct groups?\n",
    "- What might these groups mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_scatter(model_doc_topic, topic_x_id, topic_y_id):\n",
    "    # create a scatter plot of profiles with each axis representing one topic\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.loglog(model_doc_topic[:,topic_x_id], model_doc_topic[:,topic_y_id], '.', markersize=1)\n",
    "    plt.xlabel('Amount of topic {}'.format(topic_x_id))\n",
    "    plt.ylabel('Amount of topic {}'.format(topic_y_id))\n",
    "\n",
    "profile_scatter(model_doc_topic, topic_x_id=2, topic_y_id=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Interpret these topics\n",
    "- This part is for you to do: code can't do it for you.\n",
    "- Look at the list of important words for each topic, and think about these questions.\n",
    "    - What do the words have in common?\n",
    "    - What could someone write that would use most of those words?\n",
    "    - What does this topic seem to be about?\n",
    "- Try to come up with a short, catchy name for each topic.\n",
    "    - For example, if the words were \"san francisco city moved living born years raised lived live\", you might call it \"places lived\" because the topic seems to be about where people currently live (San Francisco) and where they were born / raised / moved from. \n",
    "- Try other numbers of topics.\n",
    "    - If the topics seem repetitive, you might want to try looking for fewer topics.\n",
    "    - If the topics seem confusing or vague, you might want to try looking for more topics (so that they can be more specific).\n",
    "    \n",
    "### Step 6: Check whether your interpretations match with the text\n",
    "\n",
    "#### Look at the text of a profile that has a lot of a particular topic\n",
    "- This function randomly picks one of the top few profiles for a topic, so each time you run it you will see a different example.\n",
    "    - If you want it to pick from more or less topics, change the value of `pick_from`\n",
    "    - If you want to see a different topic, change the value of `topic_a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_profiles_from_topics(documents, model_doc_topic, topic_a=8, pick_from=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the text of a profile that matches two topics well at the same time\n",
    "- Note that some topics might not happen together very often. If this is the case, the examples we find of both together might not be very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_profiles_from_topics(documents, model_doc_topic, topic_a=8, topic_b=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Topic popularity\n",
    "\n",
    "#### Helper functions to visualize and compare topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_histogram(model_doc_topic, topic_id):\n",
    "    # get profile values within a single topic\n",
    "    values = model_doc_topic[:,topic_id]\n",
    "    # calculate logarithmic bins based on smallest nonzero value\n",
    "    bins = np.logspace(np.log10(min([v for v in values if v > 0])),np.log10(0.4),50)\n",
    "    # plot the histogram\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.ylabel('Number of profiles')\n",
    "    plt.xlabel('Amount of topic {} in profile'.format(topic_id))\n",
    "    plt.hist(values, bins=bins)\n",
    "    plt.gca().set_xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall most common topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity = pd.DataFrame(model_doc_topic).mean()\n",
    "popularity = popularity.sort_values(ascending=False)\n",
    "popularity = popularity.rename_axis('Topic')\n",
    "popularity.plot.bar(title='Overall topic popularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the topic breakdown of particular profiles\n",
    "Examine different profiles by changing the value of the `pid` varible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_profile(model_doc_topic, profile_id):\n",
    "    # plot a stem diagram for a single profile\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.xticks(range(model_doc_topic.shape[1]))\n",
    "    plt.xlabel('Topic number')\n",
    "    plt.ylabel('How well this profile matches each topic')\n",
    "    plt.stem(model_doc_topic[profile_id,:])\n",
    "\n",
    "pid = 4\n",
    "\n",
    "print(documents[pid])\n",
    "visualize_profile(model_doc_topic, profile_id=pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lda_topics, feature_names=tf_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 NMF\n",
    "- NMF stands for Non-Negative Matrix Factorization. Just like you may have found factors in algebra class, this finds factors for the matrix of numbers representing word frequency in text. For example:\n",
    "    - $ 10 $ can be factored as $ 2 * 5 $\n",
    "    - $ x^2+3x+2 $ can be factored as $ (x+2)(x+1)$\n",
    "- It turns out that finding factors for text is a really good way of finding topics. This makes sense intuitively: factors are simple things we can combine to get the more complicated output, and topics are simple things people combine to write profiles.\n",
    "- [Learn more](https://en.wikipedia.org/wiki/Non-Negative_matrix_factorization#Text_mining) about NMF.\n",
    "\n",
    "\n",
    "### Step 1: Convert text to numbers the computer understands\n",
    "- NMF takes \"tf-idf vectors\" as input. Tf-idf stands for \"text frequency - inverse document frequency.\" \n",
    "    - Text frequency is the same as the count vectors above: how often does each word appear in the text?\n",
    "    - Inverse document frequency means we divide (\"inverse\") by the number of documents the word is in. (If everyone uses the word, it isn't very helpful for figuring out what makes people different. So this measurement looks for words that are used a lot in some documents, and not at all in others.)\n",
    "    - Here we tell it to only use the 1,000 most popular words, ignoring stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "print(\"Vectorizing text by TF-IDF...\")\n",
    "tfidf_text = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "tmp = tfidf_text.get_shape()\n",
    "print(\"Our transformed text has\", tmp[0], \"rows and\", tmp[1], \"columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The features are mostly the same as count vectors, because they are just the common words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print(\"The first few words (alphabetically) are:\\n\", tfidf_feature_names[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The values are different: the counts have been divided by the documents they show up in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 11\n",
    "\n",
    "print('Profile text:\\n', documents[n])\n",
    "print('\\nTF (count) vector:')\n",
    "show_vector(tfidf_text[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build a topic model using NMF\n",
    "\n",
    "- NMF is faster than LDA and often works a little better for small documents like we have here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=ntopics, alpha=.1, l1_ratio=.5, init='nndsvd')\n",
    "\n",
    "print('Performing NMF on vectors...')\n",
    "nmf = model.fit(tfidf_text)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Show our topics with the top words in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_topics = sort_topics(nmf.components_)\n",
    "display_topics(nmf_topics, tfidf_feature_names, nshow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Compare topics to each other\n",
    "We can compare topics visually by plotting the similarity of each topic to each other topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topics(nmf_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the words that make two topics similar or different\n",
    "We can also compare two topics to each other by looking at words that are common in both, or words that are common in one but not the other. Try changing topic_a and topic_b to different topic numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_a = 0\n",
    "topic_b = 1\n",
    "\n",
    "describe_intersection(nmf_topics, topic_a, topic_b, tfidf_feature_names, n_words=nshow)\n",
    "describe_difference(nmf_topics, topic_a, topic_b, tfidf_feature_names, n_words=nshow)\n",
    "describe_difference(nmf_topics, topic_b, topic_a, tfidf_feature_names, n_words=nshow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 5: Interpret these topics\n",
    "- This part is for you to do: code can't do it for you.\n",
    "- Look at the list of important words for each topic, and think about these questions.\n",
    "    - What do the words have in common?\n",
    "    - What could someone write that would use most of those words?\n",
    "    - What does this topic seem to be about?\n",
    "- Try to come up with a short, catchy name for each topic.\n",
    "    - For example, if the words were \"san francisco city moved living born years raised lived live\", you might call it \"places lived\" because the topic seems to be about where people currently live (San Francisco) and where they were born / raised / moved from. \n",
    "- Try other numbers of topics.\n",
    "    - If the topics seem repetitive, you might want to try looking for fewer topics.\n",
    "    - If the topics seem confusing or vague, you might want to try looking for more topics (so that they can be more specific)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 6: Compare the topics from LDA and NMF\n",
    "We can compare the topics visually using a confusion matrix plot.\n",
    "The NMF topics are along the X axis and the LDA are along the Y axis.\n",
    "The NMF topics are sorted to match the closest LDA topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion(x=nmf_topics, y=lda_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the LDA and NMF topic words and the confusion matrix, consider the following questions:\n",
    "- Do any of the topics seem to be the same in both models?\n",
    "- Are some topics in one model but not the other?\n",
    "- Do the topics you get from one of the models make more sense than the ones you get from the other?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To use LDA instead, remove the `#` at the beginning of the second line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, text = nmf, tfidf_text\n",
    "model_doc_topic = model.transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Who says what?\n",
    "Let's examine which topics are common to which groups of people\n",
    "\n",
    "#### Some more helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_topics(data, trait, value, n_top_topics, how='mean'):\n",
    "    topics = [col for col in data if col.startswith('topic_')]\n",
    "    data = data[data[trait] == value]\n",
    "    vals = {}\n",
    "    \n",
    "    for t in topics:\n",
    "        if how == 'mean':\n",
    "            vals[t] = data[t].mean()\n",
    "        elif how == 'count':\n",
    "            vals[t] = data[data[t] > 0][t].count()\n",
    "    vals = pd.DataFrame.from_dict(vals, orient='index')\n",
    "    vals = vals.sort_values(by=0, ascending=False).head(n_top_topics)\n",
    "    \n",
    "    return list(vals.index.values)\n",
    "\n",
    "def rank_groups(data, trait, topic, how='mean'):\n",
    "    groups = data[trait].value_counts().index.values\n",
    "    result = {}\n",
    "    \n",
    "    for g in groups:\n",
    "        if how == 'mean':\n",
    "            result[g] = data[data[trait] == g][topic].mean()\n",
    "        elif how == 'count':\n",
    "            tmp = data[data[trait] == g]\n",
    "            result[g] = tmp[tmp[topic] > 0][topic].count() / tmp.shape[0]\n",
    "    \n",
    "    r = pd.DataFrame.from_dict(result, orient='index')\n",
    "    r.columns = [topic]\n",
    "    r = r.sort_values(by=topic, ascending=False)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Merge our information about topics with our information about people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = pd.DataFrame(model_doc_topic).add_prefix('topic_')\n",
    "together = profiles.merge(topic_info, left_index=True, right_index=True)\n",
    "together.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2a: See the topics that take up the most text for a given group\n",
    "- This example shows top topics for different education levels.\n",
    "- You can change the arguments to compare different groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show what information we have about people\n",
    "together.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show what categories we have for education\n",
    "together['edu'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show most popular topics for High School graduates\n",
    "top_topics(data=together, trait='edu', value='HS', n_top_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show most popular topics for people with graduate and professional degrees\n",
    "top_topics(together, 'edu', 'Grad_pro', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show our topics again so we know what they are\n",
    "display_topics(nmf_topics, tfidf_feature_names, nshow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2b: See the topics that show up even a little in the most profiles for a given group\n",
    "- Compare these with the ones above. How are they different? Why might that be? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show most popular topics for High School graduates\n",
    "top_topics(together, 'edu', 'HS', 3, how='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show most popular topics for people with graduate and professional degrees\n",
    "top_topics(together, 'edu', 'Grad_pro', 3, 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use info theory measure to find predictive topics\n",
    "\n",
    "#### Step 3a: See the groups that have the most text about a given topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_groups(together, trait='edu', topic='topic_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3b: See the groups where the most profiles mention a given topic\n",
    "- Compare these with the results from 3a. Are they different? If so, how? Why might that be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_groups(together, trait='edu', topic='topic_2', how='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection: What we've learned\n",
    "- Two statistical methods for topic modeling\n",
    "    - LDA\n",
    "    - NMF\n",
    "- Two ways to represent text as numbers\n",
    "    - TF / count vectors (counts of how often each word is used)\n",
    "    - TF-IDF vectors (counts of how often each word is used, divided by the number of documents they're used in)\n",
    "- How to think about and interpret the topics our models find\n",
    "- How to compare and relate different topics\n",
    "- Different ways to see the distribution of topics in profiles\n",
    "- Which topics are most popular with social categories of people\n",
    "- Which social categories of people discuss a topic most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
