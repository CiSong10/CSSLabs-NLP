{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's in an online dating profile? \n",
    "\n",
    "People say a lot about themselves in online dating profiles, especially on sites like OKCupid that encourage people to answer questions. Thus, we can learn a lot about people by studying what they write. OKC has made some of their profile data from San Fransisco public. We will be using that data in this lab to explore different cultural questions. \n",
    "\n",
    "Our first question is whether and how men and women talk about themselves differently in their profiles. Popular culture is constantly telling us that men and women have different interests, hobbies, and relationship goals. Yet there are also many examples of women who like stereotypically masculine things and men who like feminine ones. This is especially interesting in online dating, because people are seeking partners with similar interests and relationship goals. Finding a partner would be hard for straight men and women if these two groups had very different interests. \n",
    "\n",
    "OKC shared 59,946 profiles though -- way too many to read! Computers can read them all and tell us how common different words are. So our first approach will be simple. We can ask \n",
    "1. Which words are used the most by men and women? \n",
    "2. Which words are used often by men but not women, and vice versa? \n",
    "\n",
    "At the end of the lab, you'll be able to ask this question about other social groups too (like sexual orientation, race/ethnicity, age, level of education, even whether someone likes dogs or cats)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "### Import the libraries we'll use.\n",
    "`%matplotlib inline` lets us see charts and plots right here in the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read our data.\n",
    "**Before you run this code:** make sure that you have downloaded the data.\n",
    "- Go to [https://github.com/rudeboybert/JSE_OkCupid](https://github.com/rudeboybert/JSE_OkCupid). \n",
    "    - If you're new to github, the easiest way is to right-click each file and \"save link as.\" You can also clone or download the whole repository. \n",
    "- Download the `okcupid_codebook.txt` and `profiles.csv.zip` files and save them in the `data` directory (folder).\n",
    "- Unzip the profiles file in the same place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = pd.read_csv('data/profiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59946, 31)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show how many rows and columns the data has\n",
    "profiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'body_type', 'diet', 'drinks', 'drugs', 'education', 'essay0',\n",
       "       'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7',\n",
       "       'essay8', 'essay9', 'ethnicity', 'height', 'income', 'job',\n",
       "       'last_online', 'location', 'offspring', 'orientation', 'pets',\n",
       "       'religion', 'sex', 'sign', 'smokes', 'speaks', 'status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the names of the columns\n",
    "profiles.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>body_type</th>\n",
       "      <th>diet</th>\n",
       "      <th>drinks</th>\n",
       "      <th>drugs</th>\n",
       "      <th>education</th>\n",
       "      <th>essay0</th>\n",
       "      <th>essay1</th>\n",
       "      <th>essay2</th>\n",
       "      <th>essay3</th>\n",
       "      <th>...</th>\n",
       "      <th>location</th>\n",
       "      <th>offspring</th>\n",
       "      <th>orientation</th>\n",
       "      <th>pets</th>\n",
       "      <th>religion</th>\n",
       "      <th>sex</th>\n",
       "      <th>sign</th>\n",
       "      <th>smokes</th>\n",
       "      <th>speaks</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>a little extra</td>\n",
       "      <td>strictly anything</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>working on college/university</td>\n",
       "      <td>about me:&lt;br /&gt;\\n&lt;br /&gt;\\ni would love to think...</td>\n",
       "      <td>currently working as an international agent fo...</td>\n",
       "      <td>making people laugh.&lt;br /&gt;\\nranting about a go...</td>\n",
       "      <td>the way i look. i am a six foot half asian, ha...</td>\n",
       "      <td>...</td>\n",
       "      <td>south san francisco, california</td>\n",
       "      <td>doesn&amp;rsquo;t have kids, but might want them</td>\n",
       "      <td>straight</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>agnosticism and very serious about it</td>\n",
       "      <td>m</td>\n",
       "      <td>gemini</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>english</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>average</td>\n",
       "      <td>mostly other</td>\n",
       "      <td>often</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>working on space camp</td>\n",
       "      <td>i am a chef: this is what that means.&lt;br /&gt;\\n1...</td>\n",
       "      <td>dedicating everyday to being an unbelievable b...</td>\n",
       "      <td>being silly. having ridiculous amonts of fun w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oakland, california</td>\n",
       "      <td>doesn&amp;rsquo;t have kids, but might want them</td>\n",
       "      <td>straight</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>agnosticism but not too serious about it</td>\n",
       "      <td>m</td>\n",
       "      <td>cancer</td>\n",
       "      <td>no</td>\n",
       "      <td>english (fluently), spanish (poorly), french (...</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>thin</td>\n",
       "      <td>anything</td>\n",
       "      <td>socially</td>\n",
       "      <td>NaN</td>\n",
       "      <td>graduated from masters program</td>\n",
       "      <td>i'm not ashamed of much, but writing public te...</td>\n",
       "      <td>i make nerdy software for musicians, artists, ...</td>\n",
       "      <td>improvising in different contexts. alternating...</td>\n",
       "      <td>my large jaw and large glasses are the physica...</td>\n",
       "      <td>...</td>\n",
       "      <td>san francisco, california</td>\n",
       "      <td>NaN</td>\n",
       "      <td>straight</td>\n",
       "      <td>has cats</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>pisces but it doesn&amp;rsquo;t matter</td>\n",
       "      <td>no</td>\n",
       "      <td>english, french, c++</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>thin</td>\n",
       "      <td>vegetarian</td>\n",
       "      <td>socially</td>\n",
       "      <td>NaN</td>\n",
       "      <td>working on college/university</td>\n",
       "      <td>i work in a library and go to school. . .</td>\n",
       "      <td>reading things written by old dead people</td>\n",
       "      <td>playing synthesizers and organizing books acco...</td>\n",
       "      <td>socially awkward but i do my best</td>\n",
       "      <td>...</td>\n",
       "      <td>berkeley, california</td>\n",
       "      <td>doesn&amp;rsquo;t want kids</td>\n",
       "      <td>straight</td>\n",
       "      <td>likes cats</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>pisces</td>\n",
       "      <td>no</td>\n",
       "      <td>english, german (poorly)</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>athletic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>graduated from college/university</td>\n",
       "      <td>hey how's it going? currently vague on the pro...</td>\n",
       "      <td>work work work work + play</td>\n",
       "      <td>creating imagery to look at:&lt;br /&gt;\\nhttp://bag...</td>\n",
       "      <td>i smile a lot and my inquisitive nature</td>\n",
       "      <td>...</td>\n",
       "      <td>san francisco, california</td>\n",
       "      <td>NaN</td>\n",
       "      <td>straight</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>aquarius</td>\n",
       "      <td>no</td>\n",
       "      <td>english</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age       body_type               diet    drinks      drugs  \\\n",
       "0   22  a little extra  strictly anything  socially      never   \n",
       "1   35         average       mostly other     often  sometimes   \n",
       "2   38            thin           anything  socially        NaN   \n",
       "3   23            thin         vegetarian  socially        NaN   \n",
       "4   29        athletic                NaN  socially      never   \n",
       "\n",
       "                           education  \\\n",
       "0      working on college/university   \n",
       "1              working on space camp   \n",
       "2     graduated from masters program   \n",
       "3      working on college/university   \n",
       "4  graduated from college/university   \n",
       "\n",
       "                                              essay0  \\\n",
       "0  about me:<br />\\n<br />\\ni would love to think...   \n",
       "1  i am a chef: this is what that means.<br />\\n1...   \n",
       "2  i'm not ashamed of much, but writing public te...   \n",
       "3          i work in a library and go to school. . .   \n",
       "4  hey how's it going? currently vague on the pro...   \n",
       "\n",
       "                                              essay1  \\\n",
       "0  currently working as an international agent fo...   \n",
       "1  dedicating everyday to being an unbelievable b...   \n",
       "2  i make nerdy software for musicians, artists, ...   \n",
       "3          reading things written by old dead people   \n",
       "4                         work work work work + play   \n",
       "\n",
       "                                              essay2  \\\n",
       "0  making people laugh.<br />\\nranting about a go...   \n",
       "1  being silly. having ridiculous amonts of fun w...   \n",
       "2  improvising in different contexts. alternating...   \n",
       "3  playing synthesizers and organizing books acco...   \n",
       "4  creating imagery to look at:<br />\\nhttp://bag...   \n",
       "\n",
       "                                              essay3    ...      \\\n",
       "0  the way i look. i am a six foot half asian, ha...    ...       \n",
       "1                                                NaN    ...       \n",
       "2  my large jaw and large glasses are the physica...    ...       \n",
       "3                  socially awkward but i do my best    ...       \n",
       "4            i smile a lot and my inquisitive nature    ...       \n",
       "\n",
       "                          location  \\\n",
       "0  south san francisco, california   \n",
       "1              oakland, california   \n",
       "2        san francisco, california   \n",
       "3             berkeley, california   \n",
       "4        san francisco, california   \n",
       "\n",
       "                                      offspring orientation  \\\n",
       "0  doesn&rsquo;t have kids, but might want them    straight   \n",
       "1  doesn&rsquo;t have kids, but might want them    straight   \n",
       "2                                           NaN    straight   \n",
       "3                       doesn&rsquo;t want kids    straight   \n",
       "4                                           NaN    straight   \n",
       "\n",
       "                        pets                                  religion sex  \\\n",
       "0  likes dogs and likes cats     agnosticism and very serious about it   m   \n",
       "1  likes dogs and likes cats  agnosticism but not too serious about it   m   \n",
       "2                   has cats                                       NaN   m   \n",
       "3                 likes cats                                       NaN   m   \n",
       "4  likes dogs and likes cats                                       NaN   m   \n",
       "\n",
       "                                 sign     smokes  \\\n",
       "0                              gemini  sometimes   \n",
       "1                              cancer         no   \n",
       "2  pisces but it doesn&rsquo;t matter         no   \n",
       "3                              pisces         no   \n",
       "4                            aquarius         no   \n",
       "\n",
       "                                              speaks     status  \n",
       "0                                            english     single  \n",
       "1  english (fluently), spanish (poorly), french (...     single  \n",
       "2                               english, french, c++  available  \n",
       "3                           english, german (poorly)     single  \n",
       "4                                            english     single  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the first few rows of data\n",
    "profiles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### First, a little housekeeping...\n",
    "You don't need to worry about the code here right now. Just run it and continue reading below. Expand for more details on how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- The OKC data has 10 different columns with profile text, one for each long-answer question in users' profiles. We want to look at all of the profile text, so this merges it all together in a new column called `text`.\n",
    "- This code also simplifies the categories people pick for other things like level of education, the pets they have, etc.\n",
    "- It removes people under 18 and over 60.\n",
    "- It saves this cleaner version of the data so we can use it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code\n",
    "This cell tells python a bunch of information about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_cols = ['essay0', 'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', \n",
    "              'essay7', 'essay8', 'essay9']\n",
    "\n",
    "ed_levels = {'<HS': ['dropped out of high school', 'working on high school'],\n",
    "             'HS': ['graduated from high school', 'working on college/university', \n",
    "                    'two-year college', 'dropped out of college/university', \n",
    "                    'high school'], \n",
    "             'BA': ['graduated from college/university', \n",
    "                    'working on masters program', 'working on ph.d program', \n",
    "                    'college/university', 'working on law school', \n",
    "                    'dropped out of masters program', \n",
    "                    'dropped out of ph.d program', 'dropped out of law school', \n",
    "                    'dropped out of med school'],\n",
    "             'Grad_Pro': ['graduated from masters program',\n",
    "                          'graduated from ph.d program',                           \n",
    "                          'graduated from law school', \n",
    "                          'graduated from med school', 'masters program', \n",
    "                          'ph.d program', 'law school', 'med school']\n",
    "            }\n",
    "\n",
    "bodies = {'average': ['average'], \n",
    "          'fit': ['fit', 'athletic', 'jacked'], \n",
    "          'thin': ['thin', 'skinny'], \n",
    "          'overweight': ['curvey', 'a little extra', 'full figured', 'overweight']\n",
    "         }\n",
    "\n",
    "smoke = {'no': ['no'], np.nan: ['nan']}\n",
    "\n",
    "kids = {'yes': ['has a kid', 'has kids']}\n",
    "\n",
    "has_pets = {'yes': ['has']}\n",
    "\n",
    "ethn = {'White': ['white', 'middle eastern', 'middle eastern, white'], \n",
    "        'Asian': ['asian', 'indian', 'asian, pacific islander'], \n",
    "        'Black': ['black']\n",
    "       }   \n",
    "\n",
    "ethn2 = {'Latinx': ['latin'], 'multiple': [','], np.nan: ['nan']}   \n",
    "\n",
    "drinks = {'no': ['rarely', 'not at all']}\n",
    "\n",
    "drugs = {'no': ['never']}\n",
    "\n",
    "jobs = {'education': ['student', 'education'], \n",
    "        'STEM': ['science', 'computer'], \n",
    "        'business': ['sales', 'executive', 'banking'], \n",
    "        'creative': ['artistic', 'entertainment'], \n",
    "        'med_law': ['medicine', 'law'],\n",
    "        np.nan: ['nan']\n",
    "       }\n",
    "\n",
    "religion = {'none': ['agnosticism', 'atheism'],\n",
    "            'catholicism': ['catholicism'],\n",
    "            'christianity': ['christianity'],\n",
    "            'judaism': ['judaism'],\n",
    "            'buddhism': ['buddhism'],\n",
    "            np.nan: ['nan']\n",
    "           }\n",
    "\n",
    "languages = {'multiple': [',']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines some functions we'll use to clean up the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(row, cols):\n",
    "    tmp = []\n",
    "    for c in cols:\n",
    "        tmp.append(str(row[c]))\n",
    "    new = '\\n'.join(tmp)\n",
    "    return new\n",
    "\n",
    "def recode(text, dictionary, default=np.nan):\n",
    "    '''Function for recoding categories in a column based on exact matches'''\n",
    "    out = default\n",
    "    text = str(text)\n",
    "    \n",
    "    for x in dictionary.keys():\n",
    "        for y in dictionary[x]:\n",
    "            if y == text: #exact match\n",
    "                out = x\n",
    "                return out\n",
    "    return out\n",
    "\n",
    "def recode_fuzzy(text, dictionary, default=np.nan):\n",
    "    '''Function for recoding categories in a column based on partial matches'''\n",
    "    out = default\n",
    "    text = str(text)\n",
    "    \n",
    "    for x in dictionary.keys():\n",
    "        for y in dictionary[x]:\n",
    "            if y in text: #partial match\n",
    "                out = x\n",
    "                return out\n",
    "    return out\n",
    "\n",
    "\n",
    "def which_pets(t, criterion='has'):\n",
    "    '''Function for determining which pets someone has or likes'''\n",
    "    d = False\n",
    "    c = False\n",
    "    t = str(t)\n",
    "    p = 'neither'\n",
    "    if t == 'nan':\n",
    "        p = np.nan\n",
    "    \n",
    "    if 'has dogs' in t:\n",
    "        d = True\n",
    "    if 'has cats' in t:\n",
    "        c = True\n",
    "        \n",
    "    if criterion == 'likes':\n",
    "        if 'likes dogs' in t:\n",
    "            if 'dislikes dogs' not in t:\n",
    "                d = True\n",
    "        if 'likes cats' in t:\n",
    "            if 'dislikes cats' not in t:\n",
    "                c = True\n",
    "        \n",
    "    if c and d:\n",
    "        p = 'both'\n",
    "    elif c:\n",
    "        p = 'cats'\n",
    "    elif d:\n",
    "        p = 'dogs'\n",
    "        \n",
    "    return p\n",
    "\n",
    "def census_2010_ethnicity(t):\n",
    "    '''\n",
    "    Function gathers choices for this question gathered by the US Census 2010.\n",
    "    It deviates from the census by creating exclusive Latinx category. Selecting \n",
    "    just 'latin' and nothing else was the 3rd most frequent ethnicity in this \n",
    "    data. The discision to include people who identified 'latin' and another race\n",
    "    is based in research on Latinx people's experience with the US Census, but \n",
    "    like all racial and ethnic categorization systems, it is flawed. \n",
    "    '''\n",
    "    text = str(t)\n",
    "    \n",
    "    e = recode(text, ethn, default='other')\n",
    "    if 'other' == e:\n",
    "        e = recode_fuzzy(text, ethn2, default='other')\n",
    "    \n",
    "    return e\n",
    "\n",
    "def height(inches):\n",
    "    h = 'under_6'\n",
    "    if inches >= 72:\n",
    "        h = 'over_6'\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the clean up\n",
    "This cell calls the functions we created in the last cell, along with the information about our data from the cell before it, to actually clean our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_group</th>\n",
       "      <th>age</th>\n",
       "      <th>body</th>\n",
       "      <th>alcohol_use</th>\n",
       "      <th>drug_use</th>\n",
       "      <th>edu</th>\n",
       "      <th>race_ethnicity</th>\n",
       "      <th>height_group</th>\n",
       "      <th>industry</th>\n",
       "      <th>kids</th>\n",
       "      <th>...</th>\n",
       "      <th>essay0</th>\n",
       "      <th>essay1</th>\n",
       "      <th>essay2</th>\n",
       "      <th>essay3</th>\n",
       "      <th>essay4</th>\n",
       "      <th>essay5</th>\n",
       "      <th>essay6</th>\n",
       "      <th>essay7</th>\n",
       "      <th>essay8</th>\n",
       "      <th>essay9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>overweight</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>HS</td>\n",
       "      <td>multiple</td>\n",
       "      <td>over_6</td>\n",
       "      <td>other</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>about me:&lt;br /&gt;\\n&lt;br /&gt;\\ni would love to think...</td>\n",
       "      <td>currently working as an international agent fo...</td>\n",
       "      <td>making people laugh.&lt;br /&gt;\\nranting about a go...</td>\n",
       "      <td>the way i look. i am a six foot half asian, ha...</td>\n",
       "      <td>books:&lt;br /&gt;\\nabsurdistan, the republic, of mi...</td>\n",
       "      <td>food.&lt;br /&gt;\\nwater.&lt;br /&gt;\\ncell phone.&lt;br /&gt;\\n...</td>\n",
       "      <td>duality and humorous things</td>\n",
       "      <td>trying to find someone to hang out with. i am ...</td>\n",
       "      <td>i am new to california and looking for someone...</td>\n",
       "      <td>you want to be swept off your feet!&lt;br /&gt;\\nyou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>average</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>White</td>\n",
       "      <td>under_6</td>\n",
       "      <td>other</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>i am a chef: this is what that means.&lt;br /&gt;\\n1...</td>\n",
       "      <td>dedicating everyday to being an unbelievable b...</td>\n",
       "      <td>being silly. having ridiculous amonts of fun w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i am die hard christopher moore fan. i don't r...</td>\n",
       "      <td>delicious porkness in all of its glories.&lt;br /...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i am very open and will share just about anyth...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>thin</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>Grad_Pro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>under_6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>i'm not ashamed of much, but writing public te...</td>\n",
       "      <td>i make nerdy software for musicians, artists, ...</td>\n",
       "      <td>improvising in different contexts. alternating...</td>\n",
       "      <td>my large jaw and large glasses are the physica...</td>\n",
       "      <td>okay this is where the cultural matrix gets so...</td>\n",
       "      <td>movement&lt;br /&gt;\\nconversation&lt;br /&gt;\\ncreation&lt;b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>viewing. listening. dancing. talking. drinking...</td>\n",
       "      <td>when i was five years old, i was known as \"the...</td>\n",
       "      <td>you are bright, open, intense, silly, ironic, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>thin</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>HS</td>\n",
       "      <td>White</td>\n",
       "      <td>under_6</td>\n",
       "      <td>education</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>i work in a library and go to school. . .</td>\n",
       "      <td>reading things written by old dead people</td>\n",
       "      <td>playing synthesizers and organizing books acco...</td>\n",
       "      <td>socially awkward but i do my best</td>\n",
       "      <td>bataille, celine, beckett. . .&lt;br /&gt;\\nlynch, j...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cats and german philosophy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>you feel so inclined.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>29</td>\n",
       "      <td>fit</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>BA</td>\n",
       "      <td>multiple</td>\n",
       "      <td>under_6</td>\n",
       "      <td>creative</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>hey how's it going? currently vague on the pro...</td>\n",
       "      <td>work work work work + play</td>\n",
       "      <td>creating imagery to look at:&lt;br /&gt;\\nhttp://bag...</td>\n",
       "      <td>i smile a lot and my inquisitive nature</td>\n",
       "      <td>music: bands, rappers, musicians&lt;br /&gt;\\nat the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  age_group  age        body alcohol_use drug_use       edu race_ethnicity  \\\n",
       "0        20   22  overweight         yes       no        HS       multiple   \n",
       "1        30   35     average         yes      yes   unknown          White   \n",
       "2        30   38        thin         yes      yes  Grad_Pro            NaN   \n",
       "3        20   23        thin         yes      yes        HS          White   \n",
       "4        20   29         fit         yes       no        BA       multiple   \n",
       "\n",
       "  height_group   industry kids  \\\n",
       "0       over_6      other   no   \n",
       "1      under_6      other   no   \n",
       "2      under_6        NaN   no   \n",
       "3      under_6  education   no   \n",
       "4      under_6   creative   no   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "2                        ...                           \n",
       "3                        ...                           \n",
       "4                        ...                           \n",
       "\n",
       "                                              essay0  \\\n",
       "0  about me:<br />\\n<br />\\ni would love to think...   \n",
       "1  i am a chef: this is what that means.<br />\\n1...   \n",
       "2  i'm not ashamed of much, but writing public te...   \n",
       "3          i work in a library and go to school. . .   \n",
       "4  hey how's it going? currently vague on the pro...   \n",
       "\n",
       "                                              essay1  \\\n",
       "0  currently working as an international agent fo...   \n",
       "1  dedicating everyday to being an unbelievable b...   \n",
       "2  i make nerdy software for musicians, artists, ...   \n",
       "3          reading things written by old dead people   \n",
       "4                         work work work work + play   \n",
       "\n",
       "                                              essay2  \\\n",
       "0  making people laugh.<br />\\nranting about a go...   \n",
       "1  being silly. having ridiculous amonts of fun w...   \n",
       "2  improvising in different contexts. alternating...   \n",
       "3  playing synthesizers and organizing books acco...   \n",
       "4  creating imagery to look at:<br />\\nhttp://bag...   \n",
       "\n",
       "                                              essay3  \\\n",
       "0  the way i look. i am a six foot half asian, ha...   \n",
       "1                                                NaN   \n",
       "2  my large jaw and large glasses are the physica...   \n",
       "3                  socially awkward but i do my best   \n",
       "4            i smile a lot and my inquisitive nature   \n",
       "\n",
       "                                              essay4  \\\n",
       "0  books:<br />\\nabsurdistan, the republic, of mi...   \n",
       "1  i am die hard christopher moore fan. i don't r...   \n",
       "2  okay this is where the cultural matrix gets so...   \n",
       "3  bataille, celine, beckett. . .<br />\\nlynch, j...   \n",
       "4  music: bands, rappers, musicians<br />\\nat the...   \n",
       "\n",
       "                                              essay5  \\\n",
       "0  food.<br />\\nwater.<br />\\ncell phone.<br />\\n...   \n",
       "1  delicious porkness in all of its glories.<br /...   \n",
       "2  movement<br />\\nconversation<br />\\ncreation<b...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                        essay6  \\\n",
       "0  duality and humorous things   \n",
       "1                          NaN   \n",
       "2                          NaN   \n",
       "3   cats and german philosophy   \n",
       "4                          NaN   \n",
       "\n",
       "                                              essay7  \\\n",
       "0  trying to find someone to hang out with. i am ...   \n",
       "1                                                NaN   \n",
       "2  viewing. listening. dancing. talking. drinking...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                              essay8  \\\n",
       "0  i am new to california and looking for someone...   \n",
       "1  i am very open and will share just about anyth...   \n",
       "2  when i was five years old, i was known as \"the...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                              essay9  \n",
       "0  you want to be swept off your feet!<br />\\nyou...  \n",
       "1                                                NaN  \n",
       "2  you are bright, open, intense, silly, ironic, ...  \n",
       "3                              you feel so inclined.  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove people 60+ and 17-\n",
    "profiles = profiles[(profiles.age < 60) & (profiles.age > 17)]\n",
    "\n",
    "#recode categorical columns into simpler categories\n",
    "profiles['text'] = profiles.apply(concat, axis=1, cols=essay_cols)\n",
    "profiles['edu'] = profiles.education.apply(recode, dictionary=ed_levels, \n",
    "                                            default='unknown')\n",
    "profiles['kids'] = profiles.offspring.apply(recode_fuzzy, dictionary=kids, \n",
    "                                            default='no')\n",
    "profiles['pets_likes'] = profiles.pets.apply(which_pets, criterion='likes')\n",
    "profiles['pets_has'] = profiles.pets.apply(which_pets, criterion='has')\n",
    "profiles['pets_any'] = profiles.pets.apply(recode_fuzzy, dictionary=has_pets, \n",
    "                                            default='no')\n",
    "profiles['age_group'] = profiles.age.apply(lambda x: str(int(x/10)*10))\n",
    "profiles['height_group'] = profiles.height.apply(height)\n",
    "profiles['race_ethnicity'] = profiles.ethnicity.apply(census_2010_ethnicity)\n",
    "profiles['smoker'] = profiles.smokes.apply(recode, dictionary=smoke, \n",
    "                                            default='yes')\n",
    "profiles['body'] = profiles.body_type.apply(recode, dictionary=bodies, \n",
    "                                            default='unknown')\n",
    "profiles['alcohol_use'] = profiles.drinks.apply(recode, dictionary=drinks, \n",
    "                                            default='yes')\n",
    "profiles['drug_use'] = profiles.drugs.apply(recode, dictionary=drugs, \n",
    "                                            default='yes')\n",
    "profiles['industry'] = profiles.job.apply(recode_fuzzy, dictionary=jobs, \n",
    "                                            default='other')\n",
    "profiles['religion'] = profiles.religion.apply(recode_fuzzy, dictionary=religion, \n",
    "                                            default='other')\n",
    "profiles['languages'] = profiles.speaks.apply(recode_fuzzy, dictionary=languages, \n",
    "                                            default='English_only')\n",
    "\n",
    "# keep just these columns\n",
    "profiles = profiles[['age_group', 'age', 'body', 'alcohol_use', 'drug_use', 'edu', \n",
    "                     'race_ethnicity', 'height_group', 'industry', 'kids', \n",
    "                     'orientation', 'pets_likes', 'pets_has', 'pets_any', \n",
    "                     'religion', 'sex', 'smoker', 'languages', 'text', 'essay0', \n",
    "                     'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', \n",
    "                     'essay7', 'essay8', 'essay9']]\n",
    "\n",
    "profiles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the results\n",
    "This cell saves the cleaned up data to a file so we can use it again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles.to_csv('data/clean_profiles.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For laptop or personal computer users\n",
    "Run this code so that you're working with a smaller amount of data and don't crash your computer. It takes a simple random sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = profiles.sample(20000)\n",
    "profiles = profiles.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### While we're at it, let's make some helper functions for later.\n",
    "Run this code, but don't worry about these now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_example(text, word, context=False):\n",
    "    #regex for selecting the whole word from a stem\n",
    "    expr = word + '\\w*'\n",
    "    \n",
    "    if context:\n",
    "        #regex for selecting a stem and also the 2 words before and after it\n",
    "        #this lets us see the context in which it is used\n",
    "        expr = '\\w*\\W*\\w*\\W*' + word + '\\w*\\W*\\w*\\W*\\w*'\n",
    "\n",
    "    return re.search(expr, text, re.I).group()\n",
    "\n",
    "def get_examples(data, word, n=5, context=True, limit_col=None, limit_val=None):\n",
    "    if word.endswith('i'):\n",
    "        #the Porter2 stemmer sometimes adds 'i' to stems. This trimms it off.\n",
    "        word = word[:-1]\n",
    "    \n",
    "    #restrict to just some group of interest\n",
    "    if limit_col is not None:\n",
    "        data = data[data[limit_col] == limit_val]\n",
    "    \n",
    "    #sample our data so this operation goes faster\n",
    "    if data.shape[0] > 1000:\n",
    "        data = data.sample(1000)\n",
    "    \n",
    "    #find profiles with the word in them\n",
    "    tmp = data.text.apply(lambda x: word in x)\n",
    "    #select n random profiles that have the word\n",
    "    count = tmp.sum()\n",
    "    \n",
    "    #if we wanted more examples than there are\n",
    "    if n > count:\n",
    "        n = count\n",
    "    tmp2 = data[tmp].text.sample(n).values\n",
    "    \n",
    "    #get an example out of each profile we selected\n",
    "    tmp = []\n",
    "    for t in tmp2:\n",
    "        tmp.append(extract_example(t, word, context))\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "def unstem(word, data, n=50):\n",
    "    if word.endswith('i'):\n",
    "        #the Porter2 stemmer sometimes adds 'i' to stems. This trimms it off.\n",
    "        word = word[:-1]\n",
    "\n",
    "    #use the function we made before to get examples of the stem\n",
    "    tmp = get_examples(data, word=word, n=n, context=False)\n",
    "    \n",
    "    #count up and return the most common form of the word matching the stem\n",
    "    return Counter(tmp).most_common(1)[0][0]\n",
    "\n",
    "def clean_index(df, text):\n",
    "    #replaces stems in the index of a dataframe with whole words\n",
    "    df.reset_index(inplace=True)\n",
    "    df['index'] = df['index'].apply(unstem, data=text)\n",
    "    df.set_index('index', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's peak at an example of the text so we know what we're working with.\n",
    "This code shows us the text for the 6th profile (python counts from 0, so the first profile is #0, the second is #1, and so on). 5 here could be any number. Try changing it to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"highly recommend not having margarita come out of your nose because\\nyou're laughing too hard- burns like a mother f'er. hate pdpg\\n(public displays of personal grooming, i.e., flossing, nail\\ncutting, etc.). freeze up when i'm the center of attention but can\\nbe reasonably charming in a small group. melt when i see men buying\\nflowers. stop at every lost pet poster and say a little prayer.\\ncan't cook worth a damn. get spanked by the big waves, but can ride\\na mean ocean ripple. own a few power tools. got soul but i'm not a\\nsoldier. appreciate the time i had with my grandfather. admire\\nwriters. have accepted that i'm not a yoga person. boulder a v3 but\\nplan on tackling v4s by year's end. (want to climb with me?) died\\nin a dream but woke up to tell about it. know i'd mess up the star\\nspangled banner if invited to perform at a baseball game. get\\nteary-eyed when i hear the song this year's love by david gray.\\nbelieve in saying please and thank you. have traveled to heaven and\\nhell but enjoy living on the earth. come across as a softy but have\\nquite a bit of bite. wonder if you're out there...\\nlet's see... started a new business which takes up a lot of my\\nenergy, yet have caught the surfing bug and am excited to carve out\\nsome time for the ocean. don't know what took me so long try it\\nout, but it seems like every few years i fall in love with a new\\nsport. i'm glad i finally made my way to the waves though.\\nrepurposing\\nnan\\nimmortality, the romantic movement, life of pi, ummm... the\\nnotebook,yes, i said it, forgetting sarah marshall, broken english,\\ndogtown, friday night lights, madmen, this american life, blind\\npilot, sean hayes, etc\\nblue<br />\\nhandyman<br />\\nsleeping in<br />\\nwine<br />\\nfriends<br />\\n#6 depends on the day\\nnan\\nnan\\numm.. i'm drinking instant starbucks coffee from one of those\\nlittle packets. yep, i said it. hideous, i know.\\nnan\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles.text[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to split the text into words so we can count them. Here's a simple first try.\n",
    "- The `split()` function, like its name suggests, splits text into chunks. If we split on spaces (the default), it will split the text into words. Let's `apply` it to the `text` of our `profiles`.\n",
    "- Notice that this is a little messy. The punctuation and some HTML things are mixed in with our words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [crap., um..., i'm, really, quiet, at, first,,...\n",
       "1    [having, fun, since, 1983., nan, nan, nan, nan...\n",
       "2    [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...\n",
       "3    [family, lives, in, menlo,, but, i, just, move...\n",
       "4    [i, am, a, person, that, is, thankful, for, ev...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = profiles['text'].apply(lambda x: x.split())\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A better way of getting words from text\n",
    "### Steps\n",
    "#### 1. Removing stop words\n",
    "- \"Stop words\" are words that are common in a language but don't tell us much about what's happening, like \"a\" or \"and.\" It is common to remove them so we can focus on more meaningful words. [Learn more](https://en.wikipedia.org/wiki/Stop_words)\n",
    "- Here we use the set of English stop words given by the NLTK library we imported above.\n",
    "- This lab makes an exception to the normal list of stop words and keeps the pronouns because some research shows that pronoun use matters in dating. You could add more words to remove or keep, depending on what you think is important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the words we will remove:\n",
      " {'that', 'those', 'will', 'shan', 'be', 'between', 'this', 'being', 'how', 'a', 'after', 'under', 's', 'was', 'over', 'do', 'there', 'not', 'but', 'are', 'wouldn', 'mightn', 'the', 'what', 'so', 'again', 'doing', 'ain', 'with', 'or', 'isn', 'any', 'ma', 'few', 'don', 'at', 'won', 'down', 'further', 'very', 'did', 'can', 'hadn', 'such', 'doesn', 'mustn', 'd', 'then', 'while', 'until', 'should', 'more', 'it', 'same', 'on', 'am', 'here', 'who', 'll', 'were', 'whom', 'because', 'no', 'during', 'couldn', 'all', 'didn', 'own', 'these', 'both', 'other', 'nor', 'when', 'against', 'in', 'which', 'wasn', 'above', 'about', 'below', 'does', 'off', 'too', 'once', 'm', 'have', 'before', 'than', 'hasn', 'o', 'itself', 'why', 'each', 't', 'haven', 'aren', 'as', 'for', 'is', 'out', 'to', 'having', 'where', 'an', 'been', 'had', 'shouldn', 'of', 'needn', 'some', 'and', 're', 'only', 'if', 'weren', 'y', 'just', 'up', 'has', 'its', 'by', 'now', 've', 'from', 'through', 'into', 'most'}\n"
     ]
    }
   ],
   "source": [
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "keep_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \n",
    "              'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "              'himself', 'she', 'her', 'hers', 'herself', 'they', 'them', 'their',\n",
    "              'theirs', 'themselves']\n",
    "\n",
    "for k in keep_words:\n",
    "    sw.discard(k) #could use remove if we wanted keyerrors\n",
    "    \n",
    "print(\"Here are the words we will remove:\\n\", sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Better tokenizing\n",
    "- This code actually cleans the text. \n",
    "    - We use the BeautifulSoup library to remove all the HTML code from the text\n",
    "    - We also remove some other non-word text like \"www\"\n",
    "    - We convert all the text to lowercase, so that the computer sees \"Dogs\", \"DOGS\", and \"dogs\" as the same word.\n",
    "    - We remove all the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [crap, um, i, really, quiet, first, i, really,...\n",
       "1    [fun, since, 1983, i, read, enough, seen, enou...\n",
       "2                                                [nan]\n",
       "3    [family, lives, menlo, i, moved, my, bff, san,...\n",
       "4    [i, person, thankful, every, day, happiest, be...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(text, sw):\n",
    "    t = BeautifulSoup(text, 'lxml').get_text()\n",
    "    \n",
    "    bad_words = ['http', 'www', '\\nnan']\n",
    "    for b in bad_words:\n",
    "        t = t.replace(b, '')\n",
    "    \n",
    "    t = t.lower()\n",
    "    t = regexp_tokenize(t, '\\w+')\n",
    "    \n",
    "    final = []\n",
    "    for w in t:\n",
    "        if w not in sw:\n",
    "            final.append(w)\n",
    "    \n",
    "    return final\n",
    "\n",
    "profiles['tokens'] = profiles['text'].apply(clean, sw=sw)\n",
    "profiles.tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Comparing the words used by men and women\n",
    "#### Step 1: We separate the profiles of women and men.\n",
    "We'll limit it to straight people for now. You'll have the chance to explore other groups later in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    [fun, since, 1983, i, read, enough, seen, enou...\n",
       "2                                                [nan]\n",
       "6    [i, big, believer, finding, happiness, whereve...\n",
       "7    [lucky, i, ran, you, everything, possible, ubu...\n",
       "9    [i, survived, one, piece, late, sixties, disco...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "men = profiles[(profiles['sex'] == 'm') & (profiles['orientation'] == 'straight')]\n",
    "women = profiles[(profiles['sex'] == 'f') & (profiles['orientation'] == 'straight')]\n",
    "\n",
    "men.tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Counting how often each gender uses each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten most common words used by men:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('i', 181351),\n",
       " ('my', 56239),\n",
       " ('you', 38121),\n",
       " ('like', 23885),\n",
       " ('me', 21067),\n",
       " ('love', 16159),\n",
       " ('good', 15279),\n",
       " ('music', 14814),\n",
       " ('people', 13180),\n",
       " ('friends', 12751)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten(series):\n",
    "    l = []\n",
    "    for x in series:\n",
    "        l.extend(x) #each x is a list we want to unnest\n",
    "    return l\n",
    "\n",
    "#\"tmp\" is often used for temporary or intermediate data that we won't use for long.\n",
    "tmp = flatten(men.tokens)\n",
    "\n",
    "print('Ten most common words used by men:')\n",
    "mens_words = Counter(tmp) #this counts how many times each word shows up\n",
    "mens_words.most_common(10) #this shows us the 10 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten most common words used by women:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('i', 128481),\n",
       " ('my', 45353),\n",
       " ('you', 24279),\n",
       " ('love', 17647),\n",
       " ('me', 15572),\n",
       " ('like', 14857),\n",
       " ('friends', 10895),\n",
       " ('good', 10317),\n",
       " ('music', 9485),\n",
       " ('people', 9141)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = flatten(women.tokens) #repeat for women\n",
    "\n",
    "print('Ten most common words used by women:')\n",
    "womens_words = Counter(tmp)\n",
    "womens_words.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the most popular words are basically the same for each gender.\n",
    "\n",
    "#### Step 3a: Put the word counts in a data frame so they're easier to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>men</th>\n",
       "      <th>women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>129.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000000001</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000x</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             men  women\n",
       "0          190.0   61.0\n",
       "00          21.0   11.0\n",
       "000        129.0   55.0\n",
       "000000001    NaN    1.0\n",
       "0000x        NaN    1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turn the two word count data into a single dataframe so it's easy to work with \n",
    "tmp = {'women': womens_words, 'men': mens_words}\n",
    "popular_words = pd.DataFrame(tmp)\n",
    "\n",
    "popular_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, the words are sorted alphabetically. That's not super useful, though.\n",
    "\n",
    "#### Step 3b: Sort the words by popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>men</th>\n",
       "      <th>women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>181351.0</td>\n",
       "      <td>128481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>56239.0</td>\n",
       "      <td>45353.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>38121.0</td>\n",
       "      <td>24279.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>16159.0</td>\n",
       "      <td>17647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me</th>\n",
       "      <td>21067.0</td>\n",
       "      <td>15572.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           men     women\n",
       "i     181351.0  128481.0\n",
       "my     56239.0   45353.0\n",
       "you    38121.0   24279.0\n",
       "love   16159.0   17647.0\n",
       "me     21067.0   15572.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_words = popular_words.sort_values(by='women', ascending=False)\n",
    "popular_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Convert those word counts to frequencies (percent of total words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>men</th>\n",
       "      <th>women</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>7.66</td>\n",
       "      <td>8.12</td>\n",
       "      <td>8.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>2.38</td>\n",
       "      <td>2.87</td>\n",
       "      <td>2.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>1.61</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.68</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>1.01</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friends</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>music</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          men  women   max\n",
       "i        7.66   8.12  8.12\n",
       "my       2.38   2.87  2.87\n",
       "you      1.61   1.53  1.61\n",
       "love     0.68   1.12  1.12\n",
       "like     1.01   0.94  1.01\n",
       "me       0.89   0.98  0.98\n",
       "friends  0.54   0.69  0.69\n",
       "good     0.65   0.65  0.65\n",
       "music    0.63   0.60  0.63\n",
       "people   0.56   0.58  0.58"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the word counts into percents (i.e. what percent of total words are x)\n",
    "popular_words['men'] = (popular_words['men'] /  popular_words['men'].sum())*100\n",
    "popular_words['women'] = (popular_words['women'] /  popular_words['women'].sum())*100\n",
    "\n",
    "#create a column \"max\" that has the word's maxmum popularity (in either men or women)\n",
    "popular_words['max'] = popular_words.max(axis=1)\n",
    "\n",
    "#show the most popular words overall\n",
    "popular_words.sort_values(by='max', ascending=False, inplace=True)\n",
    "popular_words.head(10).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see some typical examples of how these words are used\n",
    "- You can change the world `'love'` to any word you're interested in. \n",
    "- You can change the number `6` to show more or less examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of\\nhumor, love art and',\n",
       " 'come.\\ni love baseball, writing',\n",
       " \"you'll\\nlove me at\",\n",
       " 'music, i love dave matthews',\n",
       " 'it or love it.<br',\n",
       " 'some i love, but i']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_examples(data=profiles, word='love', n=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look just at examples of how men use the word 'love'\n",
    "- You can change `limit_col` to something other than `sex` if you want to look at a different attribute.\n",
    "- You can change `limit_val` to something other than `m` if you want to look at a different group within the attribute (e.g. change it to `f` if you want to see women's use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love <a class',\n",
       " 'that i love to explore',\n",
       " 'h.+p.+lovecraft\">h. p',\n",
       " 'i love travelling more',\n",
       " 'curiosity. i love learning new',\n",
       " 'home. i love it here']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_examples(data=profiles, word='love', n=6, limit_col='sex', limit_val='m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most words are very uncommon\n",
    "- The X axis in this histogram is the word popularity (percent of total words that are this word). \n",
    "- The Y axis is the number of words that have that level of popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f076ce2fe48>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f076cdc1198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show a histogram with 100 bins\n",
    "popular_words['max'].plot.hist(bins=100, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Look at just the 1000 most popular words\n",
    "- Note that the shape of the distribution looks similar, but the Y axis is much smaller ($ 10^3 $ instead of $ 10^5 $), meaning we have removed many extremely uncommon words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f076cc942b0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f076cc45ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#select only the 1000 most popular words\n",
    "popular_words = popular_words.sort_values(by='max', ascending=False).head(1000)\n",
    "\n",
    "#show the histogram again\n",
    "popular_words['max'].plot.hist(bins=100, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Figure out which words are more popular with one gender than the other\n",
    "- Here we calculate how many times different the usage of words by men or women is, so if men use a word twice as often as women use the same word, then then men's use is 2 times different. \n",
    "- Like we saw before, both groups use the most popular words about the same amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most popular words:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>men</th>\n",
       "      <th>women</th>\n",
       "      <th>max</th>\n",
       "      <th>times_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>7.665</td>\n",
       "      <td>8.119</td>\n",
       "      <td>8.119</td>\n",
       "      <td>-1.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>2.377</td>\n",
       "      <td>2.866</td>\n",
       "      <td>2.866</td>\n",
       "      <td>-1.206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>1.611</td>\n",
       "      <td>1.534</td>\n",
       "      <td>1.611</td>\n",
       "      <td>1.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.683</td>\n",
       "      <td>1.115</td>\n",
       "      <td>1.115</td>\n",
       "      <td>-1.633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>1.009</td>\n",
       "      <td>0.939</td>\n",
       "      <td>1.009</td>\n",
       "      <td>1.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me</th>\n",
       "      <td>0.890</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.984</td>\n",
       "      <td>-1.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friends</th>\n",
       "      <td>0.539</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.689</td>\n",
       "      <td>-1.278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>0.646</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.652</td>\n",
       "      <td>-1.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>music</th>\n",
       "      <td>0.626</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.626</td>\n",
       "      <td>1.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>0.557</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>-1.037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           men  women    max  times_diff\n",
       "i        7.665  8.119  8.119      -1.059\n",
       "my       2.377  2.866  2.866      -1.206\n",
       "you      1.611  1.534  1.611       1.050\n",
       "love     0.683  1.115  1.115      -1.633\n",
       "like     1.009  0.939  1.009       1.075\n",
       "me       0.890  0.984  0.984      -1.105\n",
       "friends  0.539  0.689  0.689      -1.278\n",
       "good     0.646  0.652  0.652      -1.010\n",
       "music    0.626  0.599  0.626       1.045\n",
       "people   0.557  0.578  0.578      -1.037"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def times_diff(row):\n",
    "    #calculate how many times more men use a word than women\n",
    "    #or vice versa if women use the word more\n",
    "    if row.men > row.women:\n",
    "        return row.men / row.women\n",
    "    else:\n",
    "        return -1 * (row.women / row.men)\n",
    "    \n",
    "popular_words['times_diff'] = popular_words.apply(times_diff, axis=1)\n",
    "popular_words = popular_words.sort_values(by='max', ascending=False)\n",
    "\n",
    "print('Most popular words:')\n",
    "popular_words.head(10).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the words that are most different between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words men use more than women:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>men</th>\n",
       "      <th>women</th>\n",
       "      <th>max</th>\n",
       "      <th>times_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>computers</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.020</td>\n",
       "      <td>7.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>software</th>\n",
       "      <td>0.024</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.024</td>\n",
       "      <td>5.305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineer</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.020</td>\n",
       "      <td>4.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixing</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.020</td>\n",
       "      <td>4.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startup</th>\n",
       "      <td>0.019</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.019</td>\n",
       "      <td>3.493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guitar</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.055</td>\n",
       "      <td>3.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cars</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.020</td>\n",
       "      <td>2.862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video</th>\n",
       "      <td>0.051</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.051</td>\n",
       "      <td>2.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dj</th>\n",
       "      <td>0.018</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.018</td>\n",
       "      <td>2.728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motorcycle</th>\n",
       "      <td>0.022</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.022</td>\n",
       "      <td>2.665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basketball</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.025</td>\n",
       "      <td>2.644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technology</th>\n",
       "      <td>0.029</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.029</td>\n",
       "      <td>2.558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guy</th>\n",
       "      <td>0.138</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.138</td>\n",
       "      <td>2.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>0.031</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.031</td>\n",
       "      <td>2.344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer</th>\n",
       "      <td>0.057</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.057</td>\n",
       "      <td>2.244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              men  women    max  times_diff\n",
       "computers   0.020  0.003  0.020       7.265\n",
       "software    0.024  0.005  0.024       5.305\n",
       "engineer    0.020  0.005  0.020       4.250\n",
       "fixing      0.020  0.005  0.020       4.134\n",
       "startup     0.019  0.005  0.019       3.493\n",
       "guitar      0.055  0.017  0.055       3.211\n",
       "cars        0.020  0.007  0.020       2.862\n",
       "video       0.051  0.018  0.051       2.836\n",
       "dj          0.018  0.006  0.018       2.728\n",
       "motorcycle  0.022  0.008  0.022       2.665\n",
       "basketball  0.025  0.010  0.025       2.644\n",
       "technology  0.029  0.012  0.029       2.558\n",
       "guy         0.138  0.057  0.138       2.414\n",
       "tech        0.031  0.013  0.031       2.344\n",
       "computer    0.057  0.025  0.057       2.244"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Words men use more than women:')\n",
    "popular_words.sort_values(by='times_diff', ascending=False).head(15).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words women use more than men:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>men</th>\n",
       "      <th>women</th>\n",
       "      <th>max</th>\n",
       "      <th>times_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>heels</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-10.582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baking</th>\n",
       "      <td>0.007</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-4.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yoga</th>\n",
       "      <td>0.024</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-2.661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shopping</th>\n",
       "      <td>0.007</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-2.555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>0.026</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-2.366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daughter</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-2.366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sister</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-2.307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hunger</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-2.279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>girl</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-2.272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laughter</th>\n",
       "      <td>0.027</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-2.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-2.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amelie</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-2.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passport</th>\n",
       "      <td>0.013</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-2.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compassionate</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-1.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dancing</th>\n",
       "      <td>0.068</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-1.949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 men  women    max  times_diff\n",
       "heels          0.002  0.017  0.017     -10.582\n",
       "baking         0.007  0.031  0.031      -4.705\n",
       "yoga           0.024  0.064  0.064      -2.661\n",
       "shopping       0.007  0.018  0.018      -2.555\n",
       "chocolate      0.026  0.061  0.061      -2.366\n",
       "daughter       0.008  0.020  0.020      -2.366\n",
       "sister         0.008  0.018  0.018      -2.307\n",
       "hunger         0.008  0.018  0.018      -2.279\n",
       "girl           0.055  0.126  0.126      -2.272\n",
       "laughter       0.027  0.058  0.058      -2.183\n",
       "education      0.015  0.031  0.031      -2.057\n",
       "amelie         0.008  0.017  0.017      -2.044\n",
       "passport       0.013  0.027  0.027      -2.030\n",
       "compassionate  0.008  0.017  0.017      -1.981\n",
       "dancing        0.068  0.133  0.133      -1.949"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Words women use more than men:')\n",
    "popular_words.sort_values(by='times_diff', ascending=True).head(15).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Getting cleaner results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming lets us count similar words like \"dog\" and \"dogs\" or \" run\" and \"running\" as the same word.\n",
    "- Stemming grabs just the \"stem\" of each word (the stem of both \"runs\" and \"running\" is \"run\"). When the words are converted to their stems, the computer sees them as the same. [Learn more](https://en.wikipedia.org/wiki/Stemming)\n",
    "- Stemming can be a little slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming words from profile text...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [crap, um, i, realli, quiet, first, i, realli,...\n",
       "1    [fun, sinc, 1983, i, read, enough, seen, enoug...\n",
       "2                                                [nan]\n",
       "3    [famili, live, menlo, i, move, my, bff, san, f...\n",
       "4    [i, person, thank, everi, day, happiest, beach...\n",
       "Name: stems, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#snowball English (aka porter2) is the best general stemmer\n",
    "stemmer = SnowballStemmer(\"english\") \n",
    "\n",
    "def stem(t):\n",
    "    out = []\n",
    "    for w in t:\n",
    "        out.append(stemmer.stem(w))\n",
    "    return out\n",
    "\n",
    "print(\"Stemming words from profile text...\")\n",
    "profiles['stems'] = profiles['tokens'].apply(stem)\n",
    "profiles.stems.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These functions let us do the same things we did before without rewriting all the steps each time.\n",
    "You don't have to worry about what's in them right now. Just run the cell and scroll down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for summarizing word use by a trait\n",
    "def times_diff2(row, group, ref):\n",
    "    if row[ref] > row[group]:\n",
    "        return -1 * (row[ref] / row[group])\n",
    "    else:\n",
    "        return row[group] / row[ref]\n",
    "\n",
    "#normally we wouldn't paste this function here because we already have it above\n",
    "#but it helps to show in the lab\n",
    "def flatten(series):\n",
    "    l = []\n",
    "    for x in series:\n",
    "        l.extend(x) #each x is a list we want to unnest\n",
    "    return l\n",
    "    \n",
    "def flatten2(series):\n",
    "    l = []\n",
    "    for x in series:\n",
    "        tmp = set(x) #make the tokens into a set, thus dropping repeats\n",
    "        tmp = list(tmp) #turn it back into a list we can attach to the other lists\n",
    "        l.extend(tmp) \n",
    "    return l\n",
    "\n",
    "\n",
    "def count(data, per_person):\n",
    "    #count the people in each category\n",
    "    l = len(data)\n",
    "\n",
    "    #apply the right aggregation function, depending whether we want \n",
    "    #most common words, or words used by most people\n",
    "    if per_person:\n",
    "        data = flatten2(data)\n",
    "    else:\n",
    "        data = flatten(data)\n",
    "            \n",
    "    c = Counter(data)\n",
    "    \n",
    "    return c, l\n",
    "\n",
    "def word_use(df, att, ref=None, per_person=False, undostems=False):\n",
    "    #list all of the categories in this column\n",
    "    types = list(df[att].value_counts().index.values)\n",
    "    #variables that will store our results\n",
    "    data = {}\n",
    "    lens = {}\n",
    "    \n",
    "    print(\"Counting the words used by each group...\")\n",
    "    for t in types:\n",
    "        #get the stems for each category\n",
    "        tmp = df[df[att] == t].stems\n",
    "        #count how often each is used\n",
    "        data[t], lens[t] = count(tmp, per_person)\n",
    "        \n",
    "        #also compute the inverse of each category\n",
    "        tmp = df[df[att] != t].stems\n",
    "        data['not_'+str(t)], lens['not_'+str(t)] = count(tmp, per_person)        \n",
    "        \n",
    "    #convert those results to a pandas data frame for easy handling\n",
    "    popular_words = pd.DataFrame(data)\n",
    "    \n",
    "    print('Calculating percentages...')\n",
    "    # convert the counts in each column to percents\n",
    "    for t in popular_words.columns:\n",
    "        n = lens[t] #if we want percent of people\n",
    "        \n",
    "        if not per_person: #if we want percent of total words \n",
    "            n = popular_words[t].sum()\n",
    "        \n",
    "        popular_words[t] = (popular_words[t] / n) * 100\n",
    "    \n",
    "    print('Selecting the most popular words...')\n",
    "    #find overall most popular words\n",
    "    popular_words['max'] = popular_words.max(axis=1)\n",
    "    #sort the words and select the top 1000 most popular\n",
    "    popular_words = popular_words.sort_values(by='max', ascending=False)\n",
    "    popular_words = popular_words.head(1000)\n",
    "\n",
    "    print('Calculating most distinctive words...')\n",
    "    #calculate the rate each type of person uses these words relative to others\n",
    "    for t in types:\n",
    "        r = ref\n",
    "        \n",
    "        if ref == None: #if we do not have a reference category, use the inverse\n",
    "            r = 'not_'+str(t)\n",
    "            \n",
    "        if t != ref: #don't compare a trait to itself\n",
    "            #apply our times_diff2 function\n",
    "            popular_words['times_diff_'+str(t)] = popular_words.apply(times_diff2, \n",
    "                                                                 group=t, \n",
    "                                                                 ref=r, \n",
    "                                                                 axis=1)\n",
    "\n",
    "    #remove the inverse columns we created\n",
    "    popular_words = popular_words.drop(popular_words.filter(regex='not_'), axis=1)\n",
    "    \n",
    "    if undostems:\n",
    "        print('Cleaning up word stems for readability...')\n",
    "        popular_words = clean_index(popular_words, df)\n",
    "    \n",
    "    print('Done!')\n",
    "    return popular_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try comparing men's and women's words again with stems this time\n",
    "- The top words are somewhat different now that we're counting similar words as the same.\n",
    "- We see word stems rather than whole words listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting the words used by each group...\n",
      "Calculating percentages...\n",
      "Selecting the most popular words...\n",
      "Calculating most distinctive words...\n",
      "Done!\n",
      "Men's words:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>m</th>\n",
       "      <th>max</th>\n",
       "      <th>times_diff_m</th>\n",
       "      <th>times_diff_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>softwar</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.15</td>\n",
       "      <td>-5.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engin</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.57</td>\n",
       "      <td>-3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startup</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.25</td>\n",
       "      <td>-3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guitar</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.83</td>\n",
       "      <td>-2.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comput</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.59</td>\n",
       "      <td>-2.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motorcycl</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.54</td>\n",
       "      <td>-2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technolog</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.51</td>\n",
       "      <td>-2.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basketbal</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.36</td>\n",
       "      <td>-2.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dj</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.35</td>\n",
       "      <td>-2.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2.31</td>\n",
       "      <td>-2.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f     m   max  times_diff_m  times_diff_f\n",
       "softwar    0.00  0.02  0.02          5.15         -5.15\n",
       "engin      0.01  0.04  0.04          3.57         -3.57\n",
       "startup    0.01  0.02  0.02          3.25         -3.25\n",
       "guitar     0.02  0.05  0.05          2.83         -2.83\n",
       "comput     0.03  0.08  0.08          2.59         -2.59\n",
       "motorcycl  0.01  0.03  0.03          2.54         -2.54\n",
       "technolog  0.01  0.03  0.03          2.51         -2.51\n",
       "basketbal  0.01  0.02  0.02          2.36         -2.36\n",
       "dj         0.01  0.02  0.02          2.35         -2.35\n",
       "video      0.02  0.06  0.06          2.31         -2.31"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_words = word_use(profiles, att='sex')\n",
    "popular_words = popular_words.sort_values(by='times_diff_m', ascending=False)\n",
    "print(\"Men's words:\")\n",
    "popular_words.head(10).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Those word stems in our table are a little hard to read. Let's change that.\n",
    "- The `undostems=True` option converts the stems back to whole words before showing us the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting the words used by each group...\n",
      "Calculating percentages...\n",
      "Selecting the most popular words...\n",
      "Calculating most distinctive words...\n",
      "Cleaning up word stems for readability...\n",
      "Done!\n",
      "Men's words:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>m</th>\n",
       "      <th>max</th>\n",
       "      <th>times_diff_m</th>\n",
       "      <th>times_diff_f</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>software</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.15</td>\n",
       "      <td>-5.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineer</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.57</td>\n",
       "      <td>-3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startup</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.25</td>\n",
       "      <td>-3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guitar</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.83</td>\n",
       "      <td>-2.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.59</td>\n",
       "      <td>-2.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motorcycle</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.54</td>\n",
       "      <td>-2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technology</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.51</td>\n",
       "      <td>-2.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basketball</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.36</td>\n",
       "      <td>-2.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dj</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.35</td>\n",
       "      <td>-2.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2.31</td>\n",
       "      <td>-2.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               f     m   max  times_diff_m  times_diff_f\n",
       "index                                                   \n",
       "software    0.00  0.02  0.02          5.15         -5.15\n",
       "engineer    0.01  0.04  0.04          3.57         -3.57\n",
       "startup     0.01  0.02  0.02          3.25         -3.25\n",
       "guitar      0.02  0.05  0.05          2.83         -2.83\n",
       "computer    0.03  0.08  0.08          2.59         -2.59\n",
       "motorcycle  0.01  0.03  0.03          2.54         -2.54\n",
       "technology  0.01  0.03  0.03          2.51         -2.51\n",
       "basketball  0.01  0.02  0.02          2.36         -2.36\n",
       "dj          0.01  0.02  0.02          2.35         -2.35\n",
       "video       0.02  0.06  0.06          2.31         -2.31"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_words = word_use(profiles, att='sex', undostems=True)\n",
    "popular_words = popular_words.sort_values(by='times_diff_m', ascending=False)\n",
    "print(\"Men's distinctive words:\")\n",
    "popular_words.head(10).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Women's words:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>m</th>\n",
       "      <th>max</th>\n",
       "      <th>times_diff_m</th>\n",
       "      <th>times_diff_f</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>heels</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-7.35</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bake</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-2.58</td>\n",
       "      <td>2.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yoga</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adore</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-2.26</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chicken</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-2.22</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daughter</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-2.07</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laughter</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-2.07</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hunger</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>2.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sisters</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-1.95</td>\n",
       "      <td>1.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f     m   max  times_diff_m  times_diff_f\n",
       "index                                                  \n",
       "heels      0.02  0.00  0.02         -7.35          7.35\n",
       "bake       0.04  0.02  0.04         -2.58          2.58\n",
       "yoga       0.06  0.02  0.06         -2.49          2.49\n",
       "adore      0.02  0.01  0.02         -2.26          2.26\n",
       "chicken    0.02  0.01  0.02         -2.22          2.22\n",
       "chocolate  0.06  0.03  0.06         -2.18          2.18\n",
       "daughter   0.02  0.01  0.02         -2.07          2.07\n",
       "laughter   0.06  0.03  0.06         -2.07          2.07\n",
       "hunger     0.02  0.01  0.02         -2.02          2.02\n",
       "sisters    0.03  0.01  0.03         -1.95          1.95"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_words = popular_words.sort_values(by='times_diff_f', ascending=False)\n",
    "print(\"Women's distinctive words:\")\n",
    "popular_words.head(10).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But, wait! Not all profiles have the same number of words. \n",
    "- What if a single man just wrote \"computer\" a thousand times and that is skewing our results?\n",
    "- With `per_person=True` we can see which words are used by the most different people, rather than which words are most common out of all the words used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting the words used by each group...\n",
      "Calculating percentages...\n",
      "Selecting the most popular words...\n",
      "Calculating most distinctive words...\n",
      "Cleaning up word stems for readability...\n",
      "Done!\n",
      "Men's words:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>m</th>\n",
       "      <th>max</th>\n",
       "      <th>times_diff_m</th>\n",
       "      <th>times_diff_f</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>software</th>\n",
       "      <td>0.98</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.52</td>\n",
       "      <td>-4.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineer</th>\n",
       "      <td>2.04</td>\n",
       "      <td>6.75</td>\n",
       "      <td>6.75</td>\n",
       "      <td>3.30</td>\n",
       "      <td>-3.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startup</th>\n",
       "      <td>1.21</td>\n",
       "      <td>3.80</td>\n",
       "      <td>3.80</td>\n",
       "      <td>3.14</td>\n",
       "      <td>-3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technology</th>\n",
       "      <td>2.43</td>\n",
       "      <td>5.92</td>\n",
       "      <td>5.92</td>\n",
       "      <td>2.44</td>\n",
       "      <td>-2.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guitar</th>\n",
       "      <td>3.76</td>\n",
       "      <td>8.37</td>\n",
       "      <td>8.37</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer</th>\n",
       "      <td>6.08</td>\n",
       "      <td>12.94</td>\n",
       "      <td>12.94</td>\n",
       "      <td>2.13</td>\n",
       "      <td>-2.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pulp</th>\n",
       "      <td>1.84</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.92</td>\n",
       "      <td>2.13</td>\n",
       "      <td>-2.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>2.77</td>\n",
       "      <td>5.81</td>\n",
       "      <td>5.81</td>\n",
       "      <td>2.10</td>\n",
       "      <td>-2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basketball</th>\n",
       "      <td>1.97</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3.99</td>\n",
       "      <td>2.03</td>\n",
       "      <td>-2.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motorcycle</th>\n",
       "      <td>2.39</td>\n",
       "      <td>4.79</td>\n",
       "      <td>4.79</td>\n",
       "      <td>2.00</td>\n",
       "      <td>-2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               f      m    max  times_diff_m  times_diff_f\n",
       "index                                                     \n",
       "software    0.98   4.45   4.45          4.52         -4.52\n",
       "engineer    2.04   6.75   6.75          3.30         -3.30\n",
       "startup     1.21   3.80   3.80          3.14         -3.14\n",
       "technology  2.43   5.92   5.92          2.44         -2.44\n",
       "guitar      3.76   8.37   8.37          2.22         -2.22\n",
       "computer    6.08  12.94  12.94          2.13         -2.13\n",
       "pulp        1.84   3.92   3.92          2.13         -2.13\n",
       "tech        2.77   5.81   5.81          2.10         -2.10\n",
       "basketball  1.97   3.99   3.99          2.03         -2.03\n",
       "motorcycle  2.39   4.79   4.79          2.00         -2.00"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_words = word_use(profiles, att='sex', per_person=True, undostems=True)\n",
    "print(\"Men's words:\")\n",
    "popular_words.sort_values(by='times_diff_m', ascending=False).head(10).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Women's words:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>m</th>\n",
       "      <th>max</th>\n",
       "      <th>times_diff_m</th>\n",
       "      <th>times_diff_f</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>heels</th>\n",
       "      <td>3.49</td>\n",
       "      <td>0.48</td>\n",
       "      <td>3.49</td>\n",
       "      <td>-7.33</td>\n",
       "      <td>7.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bake</th>\n",
       "      <td>8.18</td>\n",
       "      <td>3.16</td>\n",
       "      <td>8.18</td>\n",
       "      <td>-2.58</td>\n",
       "      <td>2.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yoga</th>\n",
       "      <td>10.56</td>\n",
       "      <td>4.43</td>\n",
       "      <td>10.56</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>2.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adore</th>\n",
       "      <td>3.70</td>\n",
       "      <td>1.60</td>\n",
       "      <td>3.70</td>\n",
       "      <td>-2.31</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>11.67</td>\n",
       "      <td>5.13</td>\n",
       "      <td>11.67</td>\n",
       "      <td>-2.28</td>\n",
       "      <td>2.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chicken</th>\n",
       "      <td>3.74</td>\n",
       "      <td>1.70</td>\n",
       "      <td>3.74</td>\n",
       "      <td>-2.20</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laughter</th>\n",
       "      <td>11.92</td>\n",
       "      <td>5.63</td>\n",
       "      <td>11.92</td>\n",
       "      <td>-2.12</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daughter</th>\n",
       "      <td>3.57</td>\n",
       "      <td>1.69</td>\n",
       "      <td>3.57</td>\n",
       "      <td>-2.11</td>\n",
       "      <td>2.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sister</th>\n",
       "      <td>5.87</td>\n",
       "      <td>2.83</td>\n",
       "      <td>5.87</td>\n",
       "      <td>-2.07</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hunger</th>\n",
       "      <td>3.93</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3.93</td>\n",
       "      <td>-2.06</td>\n",
       "      <td>2.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               f     m    max  times_diff_m  times_diff_f\n",
       "index                                                    \n",
       "heels       3.49  0.48   3.49         -7.33          7.33\n",
       "bake        8.18  3.16   8.18         -2.58          2.58\n",
       "yoga       10.56  4.43  10.56         -2.38          2.38\n",
       "adore       3.70  1.60   3.70         -2.31          2.31\n",
       "chocolate  11.67  5.13  11.67         -2.28          2.28\n",
       "chicken     3.74  1.70   3.74         -2.20          2.20\n",
       "laughter   11.92  5.63  11.92         -2.12          2.12\n",
       "daughter    3.57  1.69   3.57         -2.11          2.11\n",
       "sister      5.87  2.83   5.87         -2.07          2.07\n",
       "hunger      3.93  1.90   3.93         -2.06          2.06"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Women's words:\")\n",
    "popular_words.sort_values(by='times_diff_f', ascending=False).head(10).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Your turn to try it with another trait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Options (traits)\n",
    "We have a lot more information about people than just whether they're men or women. Try the analysis again with one of these other traits. (Expand for a list.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- age_group (How old someone is. Youngest users are 18.)\n",
    "    - categories: ['10', '20', '30', '40', '50']\n",
    "- body (self-described)\n",
    "    - categories: ['average', 'fit', 'thin', 'overweight', 'unknown']\n",
    "- alcohol_use\n",
    "    - categories: ['yes', 'no']\n",
    "- drug_use\n",
    "    - categories: ['yes', 'no']\n",
    "- edu (highest degree completed)\n",
    "    - categories: ['`<HS`', 'HS', 'BA', 'Grad_Pro', 'unknown'] \n",
    "- race_ethnicity\n",
    "    - categories: ['Asian', 'Black', 'Latinx', 'White', 'multiple', 'other']\n",
    "- height_group (whether someone is over or under six feet tall)\n",
    "    - categories: ['under_6', 'over_6']\n",
    "- industry (what field they work in)\n",
    "    - categories: ['STEM', 'business', 'education', 'creative', 'med_law', 'other'] \n",
    "- kids (whether they have children)\n",
    "    - categories: ['yes', 'no']\n",
    "- orientation\n",
    "    - categories: ['straight', 'gay', 'bisexual']\n",
    "- pets_likes (what pets they like)\n",
    "    - categories: ['both', 'dogs', 'cats', 'neither']\n",
    "- pets_has (what pets they have)\n",
    "    - categories: ['both', 'dogs', 'cats', 'neither']\n",
    "- pets_any (whether they have pets or not)\n",
    "    - categories: ['yes', 'no']\n",
    "- religion\n",
    "    - categories: ['christianity', 'catholicism', 'judaism', 'buddhism', 'none', 'other'] \n",
    "- sex\n",
    "    - categories: ['m', 'f']\n",
    "- smoker\n",
    "    - categories: ['yes', 'no']\n",
    "- languages\n",
    "    - categories: ['multiple', 'English_only'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to (steps)\n",
    "#### Step 1a: Decide which of the traits above you want to look at.\n",
    "#### Step 1b: Load the profile data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = pd.read_csv('data/clean_profiles.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Step 2a: If you want, limit the data to just men or women.\n",
    "- For everyone, leave this code how it is.\n",
    "- For only men, remove the `#`\n",
    "- For only women, remove the `#` and change the `'m'` in this line to `'f'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profiles = profiles[profiles['sex'] == 'm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2b: If you're running this on your personal computer\n",
    "Run this code to use just a sample of the data set, because the full data is big enough to crash most personal computers. You can make the sample bigger or smaller by changing the number here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 29)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles = profiles.sample(20000)\n",
    "profiles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Tokenize and stem the text for these profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Stemming...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing...\")\n",
    "profiles['tokens'] = profiles['text'].apply(clean, sw=sw)\n",
    "print(\"Stemming...\")\n",
    "profiles['stems'] = profiles['tokens'].apply(stem)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Compute the word usage statistics for your chosen attribute.\n",
    "You can change the code below:\n",
    "- You can change `att='age_group'` to your attribute of interest (e.g. `pets_likes` or `orientation`)\n",
    "- The `per_person` and `undostems` are the same as we saw before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting the words used by each group...\n",
      "Calculating percentages...\n",
      "Selecting the most popular words...\n",
      "Calculating most distinctive words...\n",
      "Cleaning up word stems for readability...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "result = word_use(profiles, att='age_group', per_person=True, undostems=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5a: Look at the results.\n",
    "First, let's just see what columns we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>40</th>\n",
       "      <th>50</th>\n",
       "      <th>10</th>\n",
       "      <th>max</th>\n",
       "      <th>times_diff_20</th>\n",
       "      <th>times_diff_30</th>\n",
       "      <th>times_diff_40</th>\n",
       "      <th>times_diff_50</th>\n",
       "      <th>times_diff_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>93.18</td>\n",
       "      <td>94.03</td>\n",
       "      <td>93.40</td>\n",
       "      <td>93.86</td>\n",
       "      <td>88.78</td>\n",
       "      <td>94.03</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>1.01</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>89.04</td>\n",
       "      <td>89.92</td>\n",
       "      <td>90.06</td>\n",
       "      <td>90.84</td>\n",
       "      <td>78.88</td>\n",
       "      <td>90.84</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.02</td>\n",
       "      <td>-1.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          20     30     40     50     10    max  times_diff_20  times_diff_30  \\\n",
       "index                                                                           \n",
       "i      93.18  94.03  93.40  93.86  88.78  94.03          -1.01           1.01   \n",
       "my     89.04  89.92  90.06  90.84  78.88  90.84          -1.01           1.01   \n",
       "\n",
       "       times_diff_40  times_diff_50  times_diff_10  \n",
       "index                                               \n",
       "i              -1.00           1.00          -1.05  \n",
       "my              1.01           1.02          -1.14  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head(2).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5b: Looking at the most distinctive words by category\n",
    "You can change two things in this code:\n",
    "1. Change `'times_diff_10'` to the name of the column you want to sort by, i.e. the column you want to see the most popular words in. \n",
    "2. Change the number in `head(10)` to a bigger or smaller number to see more or less rows of output.\n",
    "\n",
    "You can paste this line into more cells below and change it again to show different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>40</th>\n",
       "      <th>50</th>\n",
       "      <th>10</th>\n",
       "      <th>max</th>\n",
       "      <th>times_diff_20</th>\n",
       "      <th>times_diff_30</th>\n",
       "      <th>times_diff_40</th>\n",
       "      <th>times_diff_50</th>\n",
       "      <th>times_diff_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>idk</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>3.20</td>\n",
       "      <td>-6.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.94</td>\n",
       "      <td>4.95</td>\n",
       "      <td>4.95</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>-1.87</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.90</td>\n",
       "      <td>11.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tumblr</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.62</td>\n",
       "      <td>4.62</td>\n",
       "      <td>2.58</td>\n",
       "      <td>-2.55</td>\n",
       "      <td>-15.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transfer</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4.95</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.24</td>\n",
       "      <td>-2.40</td>\n",
       "      <td>-3.69</td>\n",
       "      <td>-6.86</td>\n",
       "      <td>7.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.80</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.96</td>\n",
       "      <td>2.39</td>\n",
       "      <td>8.58</td>\n",
       "      <td>8.58</td>\n",
       "      <td>-1.98</td>\n",
       "      <td>-1.30</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.09</td>\n",
       "      <td>7.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weeds</th>\n",
       "      <td>3.21</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.25</td>\n",
       "      <td>9.24</td>\n",
       "      <td>9.24</td>\n",
       "      <td>1.54</td>\n",
       "      <td>-1.40</td>\n",
       "      <td>-1.79</td>\n",
       "      <td>-2.15</td>\n",
       "      <td>3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haha</th>\n",
       "      <td>4.90</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>10.23</td>\n",
       "      <td>10.23</td>\n",
       "      <td>2.62</td>\n",
       "      <td>-1.82</td>\n",
       "      <td>-6.04</td>\n",
       "      <td>-6.64</td>\n",
       "      <td>3.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>college</th>\n",
       "      <td>14.67</td>\n",
       "      <td>7.26</td>\n",
       "      <td>5.95</td>\n",
       "      <td>8.01</td>\n",
       "      <td>32.01</td>\n",
       "      <td>32.01</td>\n",
       "      <td>1.90</td>\n",
       "      <td>-1.79</td>\n",
       "      <td>-1.98</td>\n",
       "      <td>-1.40</td>\n",
       "      <td>2.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wanna</th>\n",
       "      <td>5.66</td>\n",
       "      <td>3.54</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.29</td>\n",
       "      <td>12.87</td>\n",
       "      <td>12.87</td>\n",
       "      <td>1.69</td>\n",
       "      <td>-1.39</td>\n",
       "      <td>-2.35</td>\n",
       "      <td>-1.99</td>\n",
       "      <td>2.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ill</th>\n",
       "      <td>2.18</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.14</td>\n",
       "      <td>4.62</td>\n",
       "      <td>4.62</td>\n",
       "      <td>1.80</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>-1.49</td>\n",
       "      <td>2.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             20    30    40    50     10    max  times_diff_20  times_diff_30  \\\n",
       "index                                                                           \n",
       "idk        0.70  0.10   NaN   NaN   5.28   5.28           3.20          -6.04   \n",
       "1984       0.44  0.33  0.61  0.94   4.95   4.95          -1.32          -1.87   \n",
       "tumblr     0.84  0.28  0.04   NaN   4.62   4.62           2.58          -2.55   \n",
       "transfer   0.96  0.36  0.20  0.10   4.95   4.95           2.24          -2.40   \n",
       "18         0.80  1.01  1.96  2.39   8.58   8.58          -1.98          -1.30   \n",
       "weeds      3.21  2.07  1.55  1.25   9.24   9.24           1.54          -1.40   \n",
       "haha       4.90  2.15  0.61  0.52  10.23  10.23           2.62          -1.82   \n",
       "college   14.67  7.26  5.95  8.01  32.01  32.01           1.90          -1.79   \n",
       "wanna      5.66  3.54  2.04  2.29  12.87  12.87           1.69          -1.39   \n",
       "ill        2.18  1.13  1.06  1.14   4.62   4.62           1.80          -1.73   \n",
       "\n",
       "          times_diff_40  times_diff_50  times_diff_10  \n",
       "index                                                  \n",
       "idk                 NaN            NaN          14.06  \n",
       "1984               1.22           1.90          11.08  \n",
       "tumblr           -15.81            NaN           9.10  \n",
       "transfer          -3.69          -6.86           7.99  \n",
       "18                 1.78           2.09           7.86  \n",
       "weeds             -1.79          -2.15           3.67  \n",
       "haha              -6.04          -6.64           3.19  \n",
       "college           -1.98          -1.40           2.99  \n",
       "wanna             -2.35          -1.99           2.98  \n",
       "ill               -1.66          -1.49           2.84  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sort_values(by='times_diff_10', ascending=False).head(10).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>40</th>\n",
       "      <th>50</th>\n",
       "      <th>10</th>\n",
       "      <th>max</th>\n",
       "      <th>times_diff_20</th>\n",
       "      <th>times_diff_30</th>\n",
       "      <th>times_diff_40</th>\n",
       "      <th>times_diff_50</th>\n",
       "      <th>times_diff_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>graduate</th>\n",
       "      <td>11.77</td>\n",
       "      <td>3.93</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.39</td>\n",
       "      <td>4.62</td>\n",
       "      <td>11.77</td>\n",
       "      <td>3.33</td>\n",
       "      <td>-2.36</td>\n",
       "      <td>-2.93</td>\n",
       "      <td>-3.22</td>\n",
       "      <td>-1.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idk</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>3.20</td>\n",
       "      <td>-6.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haha</th>\n",
       "      <td>4.90</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>10.23</td>\n",
       "      <td>10.23</td>\n",
       "      <td>2.62</td>\n",
       "      <td>-1.82</td>\n",
       "      <td>-6.04</td>\n",
       "      <td>-6.64</td>\n",
       "      <td>3.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tumblr</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.62</td>\n",
       "      <td>4.62</td>\n",
       "      <td>2.58</td>\n",
       "      <td>-2.55</td>\n",
       "      <td>-15.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transfer</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4.95</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.24</td>\n",
       "      <td>-2.40</td>\n",
       "      <td>-3.69</td>\n",
       "      <td>-6.86</td>\n",
       "      <td>7.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potter</th>\n",
       "      <td>7.36</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.48</td>\n",
       "      <td>2.50</td>\n",
       "      <td>11.88</td>\n",
       "      <td>11.88</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>-2.28</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>2.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grad</th>\n",
       "      <td>5.63</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.66</td>\n",
       "      <td>5.63</td>\n",
       "      <td>2.13</td>\n",
       "      <td>-1.34</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-2.88</td>\n",
       "      <td>-6.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student</th>\n",
       "      <td>8.47</td>\n",
       "      <td>4.39</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.02</td>\n",
       "      <td>11.55</td>\n",
       "      <td>11.55</td>\n",
       "      <td>2.01</td>\n",
       "      <td>-1.64</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-2.12</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuck</th>\n",
       "      <td>4.97</td>\n",
       "      <td>2.93</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>-2.72</td>\n",
       "      <td>-3.05</td>\n",
       "      <td>1.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awkward</th>\n",
       "      <td>4.60</td>\n",
       "      <td>3.13</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.52</td>\n",
       "      <td>3.63</td>\n",
       "      <td>4.60</td>\n",
       "      <td>1.94</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>-4.44</td>\n",
       "      <td>-6.88</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             20    30    40    50     10    max  times_diff_20  times_diff_30  \\\n",
       "index                                                                           \n",
       "graduate  11.77  3.93  2.77  2.39   4.62  11.77           3.33          -2.36   \n",
       "idk        0.70  0.10   NaN   NaN   5.28   5.28           3.20          -6.04   \n",
       "haha       4.90  2.15  0.61  0.52  10.23  10.23           2.62          -1.82   \n",
       "tumblr     0.84  0.28  0.04   NaN   4.62   4.62           2.58          -2.55   \n",
       "transfer   0.96  0.36  0.20  0.10   4.95   4.95           2.24          -2.40   \n",
       "potter     7.36  3.45  2.48  2.50  11.88  11.88           2.18          -1.80   \n",
       "grad       5.63  3.32  1.51  1.46   0.66   5.63           2.13          -1.34   \n",
       "student    8.47  4.39  3.30  3.02  11.55  11.55           2.01          -1.64   \n",
       "fuck       4.97  2.93  1.47  1.25   5.28   5.28           1.99          -1.38   \n",
       "awkward    4.60  3.13  0.86  0.52   3.63   4.60           1.94          -1.15   \n",
       "\n",
       "          times_diff_40  times_diff_50  times_diff_10  \n",
       "index                                                  \n",
       "graduate          -2.93          -3.22          -1.62  \n",
       "idk                 NaN            NaN          14.06  \n",
       "haha              -6.04          -6.64           3.19  \n",
       "tumblr           -15.81            NaN           9.10  \n",
       "transfer          -3.69          -6.86           7.99  \n",
       "potter            -2.28          -2.17           2.29  \n",
       "grad              -2.94          -2.88          -6.25  \n",
       "student           -2.02          -2.12           1.87  \n",
       "fuck              -2.72          -3.05           1.44  \n",
       "awkward           -4.44          -6.88           1.06  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sort_values(by='times_diff_20', ascending=False).head(10).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>40</th>\n",
       "      <th>50</th>\n",
       "      <th>10</th>\n",
       "      <th>max</th>\n",
       "      <th>times_diff_20</th>\n",
       "      <th>times_diff_30</th>\n",
       "      <th>times_diff_40</th>\n",
       "      <th>times_diff_50</th>\n",
       "      <th>times_diff_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>passport</th>\n",
       "      <td>2.52</td>\n",
       "      <td>5.57</td>\n",
       "      <td>5.17</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.33</td>\n",
       "      <td>5.57</td>\n",
       "      <td>-1.98</td>\n",
       "      <td>1.92</td>\n",
       "      <td>1.43</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-11.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vacation</th>\n",
       "      <td>2.63</td>\n",
       "      <td>5.72</td>\n",
       "      <td>5.30</td>\n",
       "      <td>4.37</td>\n",
       "      <td>0.33</td>\n",
       "      <td>5.72</td>\n",
       "      <td>-2.03</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.08</td>\n",
       "      <td>-12.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balance</th>\n",
       "      <td>4.33</td>\n",
       "      <td>9.86</td>\n",
       "      <td>11.20</td>\n",
       "      <td>9.99</td>\n",
       "      <td>1.65</td>\n",
       "      <td>11.20</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.40</td>\n",
       "      <td>-4.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip</th>\n",
       "      <td>9.07</td>\n",
       "      <td>15.49</td>\n",
       "      <td>12.34</td>\n",
       "      <td>11.55</td>\n",
       "      <td>1.98</td>\n",
       "      <td>15.49</td>\n",
       "      <td>-1.54</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.07</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-5.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yoga</th>\n",
       "      <td>4.58</td>\n",
       "      <td>9.23</td>\n",
       "      <td>10.22</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0.33</td>\n",
       "      <td>10.22</td>\n",
       "      <td>-1.99</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.22</td>\n",
       "      <td>-21.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national</th>\n",
       "      <td>3.12</td>\n",
       "      <td>4.91</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.64</td>\n",
       "      <td>2.31</td>\n",
       "      <td>4.91</td>\n",
       "      <td>-1.42</td>\n",
       "      <td>1.52</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>-1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minute</th>\n",
       "      <td>3.66</td>\n",
       "      <td>5.35</td>\n",
       "      <td>3.95</td>\n",
       "      <td>2.71</td>\n",
       "      <td>0.66</td>\n",
       "      <td>5.35</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>1.50</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>-6.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mix</th>\n",
       "      <td>4.10</td>\n",
       "      <td>6.74</td>\n",
       "      <td>6.44</td>\n",
       "      <td>5.10</td>\n",
       "      <td>2.64</td>\n",
       "      <td>6.74</td>\n",
       "      <td>-1.56</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.25</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>-2.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fortunately</th>\n",
       "      <td>1.68</td>\n",
       "      <td>3.48</td>\n",
       "      <td>4.85</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.66</td>\n",
       "      <td>4.85</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.23</td>\n",
       "      <td>-4.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2.33</td>\n",
       "      <td>4.15</td>\n",
       "      <td>5.13</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>5.13</td>\n",
       "      <td>-1.77</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.71</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>-10.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               20     30     40     50    10    max  times_diff_20  \\\n",
       "index                                                                \n",
       "passport     2.52   5.57   5.17   1.77  0.33   5.57          -1.98   \n",
       "vacation     2.63   5.72   5.30   4.37  0.33   5.72          -2.03   \n",
       "balance      4.33   9.86  11.20   9.99  1.65  11.20          -2.30   \n",
       "trip         9.07  15.49  12.34  11.55  1.98  15.49          -1.54   \n",
       "yoga         4.58   9.23  10.22   8.43  0.33  10.22          -1.99   \n",
       "national     3.12   4.91   3.63   3.64  2.31   4.91          -1.42   \n",
       "minute       3.66   5.35   3.95   2.71  0.66   5.35          -1.27   \n",
       "mix          4.10   6.74   6.44   5.10  2.64   6.74          -1.56   \n",
       "fortunately  1.68   3.48   4.85   3.33  0.66   4.85          -2.21   \n",
       "80           2.33   4.15   5.13   2.50  0.33   5.13          -1.77   \n",
       "\n",
       "             times_diff_30  times_diff_40  times_diff_50  times_diff_10  \n",
       "index                                                                    \n",
       "passport              1.92           1.43          -2.21         -11.69  \n",
       "vacation              1.79           1.37           1.08         -12.44  \n",
       "balance               1.66           1.67           1.40          -4.46  \n",
       "trip                  1.60           1.07          -1.01          -5.96  \n",
       "yoga                  1.59           1.57           1.22         -21.41  \n",
       "national              1.52          -1.05          -1.04          -1.65  \n",
       "minute                1.50          -1.06          -1.57          -6.41  \n",
       "mix                   1.47           1.25          -1.04          -2.02  \n",
       "fortunately           1.47           1.98           1.23          -4.20  \n",
       "80                    1.47           1.71          -1.32         -10.03  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sort_values(by='times_diff_30', ascending=False).head(10).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>40</th>\n",
       "      <th>50</th>\n",
       "      <th>10</th>\n",
       "      <th>max</th>\n",
       "      <th>times_diff_20</th>\n",
       "      <th>times_diff_30</th>\n",
       "      <th>times_diff_40</th>\n",
       "      <th>times_diff_50</th>\n",
       "      <th>times_diff_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sensual</th>\n",
       "      <td>0.29</td>\n",
       "      <td>1.11</td>\n",
       "      <td>3.75</td>\n",
       "      <td>6.66</td>\n",
       "      <td>0.33</td>\n",
       "      <td>6.66</td>\n",
       "      <td>-7.55</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>3.91</td>\n",
       "      <td>6.47</td>\n",
       "      <td>-3.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>divorce</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.71</td>\n",
       "      <td>2.40</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0.33</td>\n",
       "      <td>4.99</td>\n",
       "      <td>-5.47</td>\n",
       "      <td>-1.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>7.10</td>\n",
       "      <td>-2.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daughter</th>\n",
       "      <td>0.77</td>\n",
       "      <td>2.07</td>\n",
       "      <td>5.66</td>\n",
       "      <td>9.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.78</td>\n",
       "      <td>-4.65</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>3.24</td>\n",
       "      <td>5.29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intimacy</th>\n",
       "      <td>0.61</td>\n",
       "      <td>1.32</td>\n",
       "      <td>3.54</td>\n",
       "      <td>6.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.04</td>\n",
       "      <td>-3.68</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>3.03</td>\n",
       "      <td>4.91</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>integrity</th>\n",
       "      <td>0.99</td>\n",
       "      <td>2.27</td>\n",
       "      <td>5.13</td>\n",
       "      <td>6.24</td>\n",
       "      <td>0.99</td>\n",
       "      <td>6.24</td>\n",
       "      <td>-3.31</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.16</td>\n",
       "      <td>-2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intimacy</th>\n",
       "      <td>0.53</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.61</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.33</td>\n",
       "      <td>4.27</td>\n",
       "      <td>-3.14</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>2.81</td>\n",
       "      <td>4.37</td>\n",
       "      <td>-3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affectionate</th>\n",
       "      <td>0.73</td>\n",
       "      <td>2.24</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.89</td>\n",
       "      <td>0.33</td>\n",
       "      <td>4.89</td>\n",
       "      <td>-3.94</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.86</td>\n",
       "      <td>-5.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generous</th>\n",
       "      <td>0.66</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3.71</td>\n",
       "      <td>4.79</td>\n",
       "      <td>0.33</td>\n",
       "      <td>4.79</td>\n",
       "      <td>-3.83</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.74</td>\n",
       "      <td>3.23</td>\n",
       "      <td>-5.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spiritual</th>\n",
       "      <td>1.80</td>\n",
       "      <td>4.62</td>\n",
       "      <td>9.29</td>\n",
       "      <td>12.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.70</td>\n",
       "      <td>-3.51</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.39</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>companion</th>\n",
       "      <td>0.74</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.81</td>\n",
       "      <td>4.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.99</td>\n",
       "      <td>-2.50</td>\n",
       "      <td>-1.23</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.36</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                20    30    40     50    10    max  times_diff_20  \\\n",
       "index                                                               \n",
       "sensual       0.29  1.11  3.75   6.66  0.33   6.66          -7.55   \n",
       "divorce       0.27  0.71  2.40   4.99  0.33   4.99          -5.47   \n",
       "daughter      0.77  2.07  5.66   9.78   NaN   9.78          -4.65   \n",
       "intimacy      0.61  1.32  3.54   6.04   NaN   6.04          -3.68   \n",
       "integrity     0.99  2.27  5.13   6.24  0.99   6.24          -3.31   \n",
       "intimacy      0.53  1.04  2.61   4.27  0.33   4.27          -3.14   \n",
       "affectionate  0.73  2.24  4.24   4.89  0.33   4.89          -3.94   \n",
       "generous      0.66  1.88  3.71   4.79  0.33   4.79          -3.83   \n",
       "spiritual     1.80  4.62  9.29  12.70   NaN  12.70          -3.51   \n",
       "companion     0.74  1.16  2.81   4.99   NaN   4.99          -2.50   \n",
       "\n",
       "              times_diff_30  times_diff_40  times_diff_50  times_diff_10  \n",
       "index                                                                     \n",
       "sensual               -1.26           3.91           6.47          -3.98  \n",
       "divorce               -1.42           3.43           7.10          -2.78  \n",
       "daughter              -1.11           3.24           5.29            NaN  \n",
       "intimacy              -1.16           3.03           4.91            NaN  \n",
       "integrity              1.06           2.90           3.16          -2.22  \n",
       "intimacy              -1.14           2.81           4.37          -3.48  \n",
       "affectionate           1.34           2.76           2.86          -5.72  \n",
       "generous               1.24           2.74           3.23          -5.03  \n",
       "spiritual              1.17           2.69           3.39            NaN  \n",
       "companion             -1.23           2.50           4.36            NaN  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sort_values(by='times_diff_40', ascending=False).head(10).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>40</th>\n",
       "      <th>50</th>\n",
       "      <th>10</th>\n",
       "      <th>max</th>\n",
       "      <th>times_diff_20</th>\n",
       "      <th>times_diff_30</th>\n",
       "      <th>times_diff_40</th>\n",
       "      <th>times_diff_50</th>\n",
       "      <th>times_diff_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>divorce</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.71</td>\n",
       "      <td>2.40</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0.33</td>\n",
       "      <td>4.99</td>\n",
       "      <td>-5.47</td>\n",
       "      <td>-1.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>7.10</td>\n",
       "      <td>-2.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retirement</th>\n",
       "      <td>0.57</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.18</td>\n",
       "      <td>5.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.72</td>\n",
       "      <td>-2.83</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.07</td>\n",
       "      <td>6.52</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sensual</th>\n",
       "      <td>0.29</td>\n",
       "      <td>1.11</td>\n",
       "      <td>3.75</td>\n",
       "      <td>6.66</td>\n",
       "      <td>0.33</td>\n",
       "      <td>6.66</td>\n",
       "      <td>-7.55</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>3.91</td>\n",
       "      <td>6.47</td>\n",
       "      <td>-3.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daughter</th>\n",
       "      <td>0.77</td>\n",
       "      <td>2.07</td>\n",
       "      <td>5.66</td>\n",
       "      <td>9.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.78</td>\n",
       "      <td>-4.65</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>3.24</td>\n",
       "      <td>5.29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intimacy</th>\n",
       "      <td>0.61</td>\n",
       "      <td>1.32</td>\n",
       "      <td>3.54</td>\n",
       "      <td>6.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.04</td>\n",
       "      <td>-3.68</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>3.03</td>\n",
       "      <td>4.91</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mutual</th>\n",
       "      <td>0.59</td>\n",
       "      <td>1.11</td>\n",
       "      <td>2.53</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.99</td>\n",
       "      <td>4.68</td>\n",
       "      <td>-3.01</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>2.48</td>\n",
       "      <td>4.55</td>\n",
       "      <td>-1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intimacy</th>\n",
       "      <td>0.53</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.61</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.33</td>\n",
       "      <td>4.27</td>\n",
       "      <td>-3.14</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>2.81</td>\n",
       "      <td>4.37</td>\n",
       "      <td>-3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>companion</th>\n",
       "      <td>0.74</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.81</td>\n",
       "      <td>4.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.99</td>\n",
       "      <td>-2.50</td>\n",
       "      <td>-1.23</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.36</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grown</th>\n",
       "      <td>1.30</td>\n",
       "      <td>1.96</td>\n",
       "      <td>2.04</td>\n",
       "      <td>6.35</td>\n",
       "      <td>0.66</td>\n",
       "      <td>6.35</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.12</td>\n",
       "      <td>3.92</td>\n",
       "      <td>-2.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>growth</th>\n",
       "      <td>0.89</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.36</td>\n",
       "      <td>4.79</td>\n",
       "      <td>0.33</td>\n",
       "      <td>4.79</td>\n",
       "      <td>-2.44</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.63</td>\n",
       "      <td>3.41</td>\n",
       "      <td>-4.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              20    30    40    50    10   max  times_diff_20  times_diff_30  \\\n",
       "index                                                                          \n",
       "divorce     0.27  0.71  2.40  4.99  0.33  4.99          -5.47          -1.42   \n",
       "retirement  0.57  1.24  1.18  5.72   NaN  5.72          -2.83           1.20   \n",
       "sensual     0.29  1.11  3.75  6.66  0.33  6.66          -7.55          -1.26   \n",
       "daughter    0.77  2.07  5.66  9.78   NaN  9.78          -4.65          -1.11   \n",
       "intimacy    0.61  1.32  3.54  6.04   NaN  6.04          -3.68          -1.16   \n",
       "mutual      0.59  1.11  2.53  4.68  0.99  4.68          -3.01          -1.13   \n",
       "intimacy    0.53  1.04  2.61  4.27  0.33  4.27          -3.14          -1.14   \n",
       "companion   0.74  1.16  2.81  4.99   NaN  4.99          -2.50          -1.23   \n",
       "grown       1.30  1.96  2.04  6.35  0.66  6.35          -1.80           1.09   \n",
       "growth      0.89  1.82  2.36  4.79  0.33  4.79          -2.44           1.27   \n",
       "\n",
       "            times_diff_40  times_diff_50  times_diff_10  \n",
       "index                                                    \n",
       "divorce              3.43           7.10          -2.78  \n",
       "retirement           1.07           6.52            NaN  \n",
       "sensual              3.91           6.47          -3.98  \n",
       "daughter             3.24           5.29            NaN  \n",
       "intimacy             3.03           4.91            NaN  \n",
       "mutual               2.48           4.55          -1.22  \n",
       "intimacy             2.81           4.37          -3.48  \n",
       "companion            2.50           4.36            NaN  \n",
       "grown                1.12           3.92          -2.82  \n",
       "growth               1.63           3.41          -4.80  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sort_values(by='times_diff_50', ascending=False).head(10).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. What we learned\n",
    "Expand for more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Sociology & Gender\n",
    "1. Overall, the most common words in online dating are the same for men and women in San Fransisco. What they say about themselves is not that different. \n",
    "2. There are some words that men use much more often than women, and vice versa. These fit stereotypical gender roles: for example, men in San Fransisco are much more likely to talk about computers, startups, engineering, and sports. And women are much more likely to talk about food (e.g. baking and chocolate) or feelings (adore, laughter). \n",
    "3. There are many possible causes for these differences in word use. For example, it is often taboo for men to talk about their feelings, so they may mention them less here because of social expectations rather than because they are less emotional. Social factors can also increase expression: for instance, women typically do the majority of food preparation in American families, so it is not surprising that they are more likely than men to talk about it in dating profiles. \n",
    "4. Not every person conforms to these broad patterns. Only 10-20% of these men mention computers. A similar percent of the women mention baking. Some women talk about computers, and some men talk about baking. Most people aren't using these very gendered words at all. What we showed is that there are broad patterns of some topics being much more popular with men or women, and that these patterns line up with common cultural expectations of gender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Text analysis\n",
    "1. **Tokenizing** is the process of splitting text into words (tokens). Simple approaches can separate words based on spaces, but punctuation, HTML, and other things can make this more complicated. \n",
    "2. **Stop words** are words that are common but don't give us much information. They're often removed before we do analysis.\n",
    "3. **Stemming** lets us combine similar words like \"runs\" and \"running\" by looking at the stem of the words (in this case, \"run\"). \n",
    "4. Most words are not very common. [Oxford Dictionaries](https://en.oxforddictionaries.com/explore/how-many-words-are-there-in-the-english-language) lists over 171,000 currently used English words, but as we saw, only a few words show up in more than a few profiles. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
